{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "784535ae-5d3f-49b7-a4f1-81043aeff347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/lm-evaluation-harness'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bda24-dc92-40f1-97c1-b1d591ddffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e3f46b-25da-4891-a3ba-e6a6b8f94dba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08:15:03:04 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-08:15:03:36 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-08:15:04:15 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_en']\n",
      "2026-01-08:15:04:15 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-08:15:04:15 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-08:15:04:15 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:04:21\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "config.json: 100%|█████████████████████████████| 679/679 [00:00<00:00, 5.38MB/s]\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:04:32\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:04:32\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:04:34\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "tokenizer_config.json: 3.07kB [00:00, 14.2MB/s]\n",
      "tokenizer.json: 7.03MB [00:00, 56.9MB/s]\n",
      "generation_config.json: 100%|██████████████████| 181/181 [00:00<00:00, 2.11MB/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:04:36\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:04:37\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:55183 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:04:37\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:04:37\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:04:58\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "model.safetensors: 100%|███████████████████| 3.55G/3.55G [00:03<00:00, 1.03GB/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:02\u001b[0m \u001b[90m[weight_utils.py:487]\u001b[0m Time spent downloading weights for deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B: 3.876592 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:02\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.85it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.85it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:03\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.60 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:03\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 25.170804 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:09\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/f195ca22f3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:09\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 4.96 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:18\u001b[0m \u001b[90m[backends.py:261]\u001b[0m Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:24\u001b[0m \u001b[90m[backends.py:278]\u001b[0m Compiling a graph for compile range (1, 8192) takes 12.57 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:24\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 17.52 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:27\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:27\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:27\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:01<00:00, 31.14it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:31\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 4 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=977)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:31\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 27.61 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:05:31\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "README.md: 15.0kB [00:00, 50.5MB/s]\n",
      "en/train-00000-of-00001.parquet: 100%|█████| 5.62k/5.62k [00:00<00:00, 9.16kB/s]\n",
      "en/test-00000-of-00001.parquet: 100%|██████| 39.9k/39.9k [00:00<00:00, 83.1kB/s]\n",
      "Generating train split: 100%|████████████| 8/8 [00:00<00:00, 1547.29 examples/s]\n",
      "Generating test split: 100%|████████| 250/250 [00:00<00:00, 56453.97 examples/s]\n",
      "2026-01-08:15:05:40 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-08:15:05:40 INFO     [tasks:691] Task: mgsm_en_cot_en (mgsm/en_cot/mgsm_en_cot_en.yaml)\n",
      "2026-01-08:15:05:40 INFO     [evaluator:313] mgsm_en_cot_en: Using gen_kwargs: {'do_sample': True, 'until': ['Question:', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-08:15:05:40 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-08:15:05:40 INFO     [api.task:310] Building contexts for mgsm_en_cot_en on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 475.64it/s]\n",
      "2026-01-08:15:05:41 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████| 250/250 [00:00<00:00, 8267.77it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:04<19:14,  4.64s/it, est. speed input: 12.7\u001b[A\n",
      "Processed prompts:   1%| | 2/250 [00:05<09:43,  2.35s/it, est. speed input: 22.8\u001b[A\n",
      "Processed prompts:   1%| | 3/250 [00:06<06:25,  1.56s/it, est. speed input: 34.3\u001b[A\n",
      "Processed prompts:   2%| | 4/250 [00:06<04:21,  1.06s/it, est. speed input: 44.1\u001b[A\n",
      "Processed prompts:   2%| | 5/250 [00:06<03:29,  1.17it/s, est. speed input: 48.8\u001b[A\n",
      "Processed prompts:   3%| | 7/250 [00:07<02:02,  1.99it/s, est. speed input: 62.9\u001b[A\n",
      "Processed prompts:   3%| | 8/250 [00:07<01:43,  2.35it/s, est. speed input: 71.7\u001b[A\n",
      "Processed prompts:   4%| | 10/250 [00:07<01:32,  2.61it/s, est. speed input: 82.\u001b[A\n",
      "Processed prompts:   4%| | 11/250 [00:08<01:40,  2.38it/s, est. speed input: 84.\u001b[A\n",
      "Processed prompts:   5%| | 12/250 [00:08<01:39,  2.39it/s, est. speed input: 90.\u001b[A\n",
      "Processed prompts:   5%| | 13/250 [00:10<02:28,  1.60it/s, est. speed input: 87.\u001b[A\n",
      "Processed prompts:   6%| | 14/250 [00:10<01:56,  2.03it/s, est. speed input: 93.\u001b[A\n",
      "Processed prompts:   6%| | 15/250 [00:10<02:01,  1.93it/s, est. speed input: 96.\u001b[A\n",
      "Processed prompts:   7%| | 17/250 [00:11<01:20,  2.90it/s, est. speed input: 116\u001b[A\n",
      "Processed prompts:   8%| | 19/250 [00:12<01:28,  2.61it/s, est. speed input: 118\u001b[A\n",
      "Processed prompts:   8%| | 20/250 [00:12<01:21,  2.81it/s, est. speed input: 124\u001b[A\n",
      "Processed prompts:   8%| | 21/250 [00:12<01:31,  2.49it/s, est. speed input: 124\u001b[A\n",
      "Processed prompts:   9%| | 22/250 [00:12<01:17,  2.96it/s, est. speed input: 128\u001b[A\n",
      "Processed prompts:   9%| | 23/250 [00:13<01:37,  2.32it/s, est. speed input: 127\u001b[A\n",
      "Processed prompts:  10%| | 24/250 [00:14<02:29,  1.51it/s, est. speed input: 120\u001b[A\n",
      "Processed prompts:  10%| | 25/250 [00:15<01:55,  1.95it/s, est. speed input: 122\u001b[A\n",
      "Processed prompts:  10%| | 26/250 [00:15<02:22,  1.57it/s, est. speed input: 121\u001b[A\n",
      "Processed prompts:  11%| | 27/250 [00:16<02:32,  1.46it/s, est. speed input: 118\u001b[A\n",
      "Processed prompts:  11%| | 28/250 [00:16<01:59,  1.86it/s, est. speed input: 123\u001b[A\n",
      "Processed prompts:  12%| | 29/250 [00:17<01:39,  2.23it/s, est. speed input: 124\u001b[A\n",
      "Processed prompts:  12%| | 31/250 [00:17<01:06,  3.28it/s, est. speed input: 130\u001b[A\n",
      "Processed prompts:  13%|▏| 32/250 [00:17<01:04,  3.38it/s, est. speed input: 134\u001b[A\n",
      "Processed prompts:  13%|▏| 33/250 [00:18<01:19,  2.74it/s, est. speed input: 135\u001b[A\n",
      "Processed prompts:  14%|▏| 35/250 [00:18<01:04,  3.34it/s, est. speed input: 142\u001b[A\n",
      "Processed prompts:  15%|▏| 38/250 [00:18<00:40,  5.26it/s, est. speed input: 152\u001b[A\n",
      "Processed prompts:  16%|▏| 41/250 [00:19<00:32,  6.42it/s, est. speed input: 160\u001b[A\n",
      "Processed prompts:  17%|▏| 43/250 [00:19<00:28,  7.28it/s, est. speed input: 167\u001b[A\n",
      "Processed prompts:  18%|▏| 44/250 [00:19<00:30,  6.70it/s, est. speed input: 169\u001b[A\n",
      "Processed prompts:  18%|▏| 46/250 [00:20<00:34,  5.96it/s, est. speed input: 172\u001b[A\n",
      "Processed prompts:  20%|▏| 49/250 [00:20<00:25,  7.87it/s, est. speed input: 182\u001b[A\n",
      "Processed prompts:  20%|▏| 50/250 [00:20<00:27,  7.37it/s, est. speed input: 183\u001b[A\n",
      "Processed prompts:  20%|▏| 51/250 [00:20<00:29,  6.71it/s, est. speed input: 185\u001b[A\n",
      "Processed prompts:  21%|▏| 53/250 [00:20<00:26,  7.53it/s, est. speed input: 191\u001b[A\n",
      "Processed prompts:  22%|▏| 54/250 [00:21<00:27,  7.04it/s, est. speed input: 192\u001b[A\n",
      "Processed prompts:  22%|▏| 55/250 [00:21<00:31,  6.16it/s, est. speed input: 193\u001b[A\n",
      "Processed prompts:  23%|▏| 58/250 [00:21<00:25,  7.49it/s, est. speed input: 200\u001b[A\n",
      "Processed prompts:  24%|▏| 59/250 [00:21<00:24,  7.79it/s, est. speed input: 204\u001b[A\n",
      "Processed prompts:  24%|▏| 61/250 [00:21<00:25,  7.32it/s, est. speed input: 211\u001b[A\n",
      "Processed prompts:  26%|▎| 65/250 [00:22<00:16, 11.46it/s, est. speed input: 224\u001b[A\n",
      "Processed prompts:  27%|▎| 67/250 [00:22<00:25,  7.07it/s, est. speed input: 226\u001b[A\n",
      "Processed prompts:  28%|▎| 69/250 [00:22<00:24,  7.25it/s, est. speed input: 232\u001b[A\n",
      "Processed prompts:  28%|▎| 70/250 [00:23<00:40,  4.47it/s, est. speed input: 230\u001b[A\n",
      "Processed prompts:  29%|▎| 72/250 [00:23<00:33,  5.39it/s, est. speed input: 236\u001b[A\n",
      "Processed prompts:  29%|▎| 73/250 [00:24<00:31,  5.60it/s, est. speed input: 238\u001b[A\n",
      "Processed prompts:  30%|▎| 74/250 [00:24<00:28,  6.14it/s, est. speed input: 240\u001b[A\n",
      "Processed prompts:  31%|▎| 77/250 [00:24<00:17,  9.78it/s, est. speed input: 247\u001b[A\n",
      "Processed prompts:  32%|▎| 79/250 [00:24<00:25,  6.82it/s, est. speed input: 248\u001b[A\n",
      "Processed prompts:  32%|▎| 81/250 [00:24<00:22,  7.53it/s, est. speed input: 254\u001b[A\n",
      "Processed prompts:  33%|▎| 83/250 [00:25<00:26,  6.31it/s, est. speed input: 255\u001b[A\n",
      "Processed prompts:  34%|▎| 84/250 [00:25<00:31,  5.34it/s, est. speed input: 257\u001b[A\n",
      "Processed prompts:  35%|▎| 88/250 [00:25<00:19,  8.28it/s, est. speed input: 267\u001b[A\n",
      "Processed prompts:  36%|▎| 90/250 [00:26<00:34,  4.61it/s, est. speed input: 264\u001b[A\n",
      "Processed prompts:  37%|▎| 92/250 [00:27<00:37,  4.17it/s, est. speed input: 266\u001b[A\n",
      "Processed prompts:  37%|▎| 93/250 [00:27<00:36,  4.32it/s, est. speed input: 267\u001b[A\n",
      "Processed prompts:  38%|▍| 96/250 [00:27<00:27,  5.69it/s, est. speed input: 272\u001b[A\n",
      "Processed prompts:  39%|▍| 98/250 [00:28<00:23,  6.52it/s, est. speed input: 276\u001b[A\n",
      "Processed prompts:  40%|▍| 100/250 [00:28<00:23,  6.25it/s, est. speed input: 28\u001b[A\n",
      "Processed prompts:  40%|▍| 101/250 [00:28<00:24,  6.08it/s, est. speed input: 28\u001b[A\n",
      "Processed prompts:  42%|▍| 104/250 [00:28<00:16,  8.71it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  42%|▍| 106/250 [00:29<00:18,  7.75it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  43%|▍| 108/250 [00:29<00:20,  6.82it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  44%|▍| 109/250 [00:29<00:23,  6.04it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  44%|▍| 110/250 [00:29<00:23,  5.86it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  45%|▍| 113/250 [00:30<00:16,  8.55it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  46%|▍| 115/250 [00:30<00:13, 10.10it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  47%|▍| 117/250 [00:31<00:24,  5.38it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  48%|▍| 120/250 [00:31<00:29,  4.42it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  48%|▍| 121/250 [00:32<00:34,  3.78it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  49%|▍| 123/250 [00:32<00:26,  4.82it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  50%|▍| 124/250 [00:32<00:24,  5.25it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  51%|▌| 127/250 [00:32<00:17,  7.15it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  52%|▌| 129/250 [00:32<00:13,  8.71it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  52%|▌| 131/250 [00:33<00:24,  4.90it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  53%|▌| 132/250 [00:33<00:22,  5.35it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  53%|▌| 133/250 [00:34<00:33,  3.52it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  54%|▌| 136/250 [00:35<00:25,  4.39it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  55%|▌| 137/250 [00:35<00:26,  4.22it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  55%|▌| 138/250 [00:35<00:24,  4.66it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  56%|▌| 139/250 [00:35<00:26,  4.13it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  56%|▌| 141/250 [00:35<00:18,  5.82it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  57%|▌| 143/250 [00:36<00:17,  6.29it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  58%|▌| 144/250 [00:36<00:23,  4.45it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  58%|▌| 145/250 [00:36<00:20,  5.08it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  58%|▌| 146/250 [00:36<00:18,  5.75it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  59%|▌| 147/250 [00:37<00:19,  5.18it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  59%|▌| 148/250 [00:37<00:17,  5.94it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  60%|▌| 149/250 [00:37<00:30,  3.31it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  60%|▌| 150/250 [00:38<00:30,  3.23it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  61%|▌| 152/250 [00:38<00:21,  4.62it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  61%|▌| 153/250 [00:38<00:24,  3.97it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  62%|▌| 154/250 [00:39<00:43,  2.23it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  62%|▌| 155/250 [00:39<00:33,  2.81it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  62%|▌| 156/250 [00:40<00:29,  3.23it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  63%|▋| 157/250 [00:40<00:30,  3.01it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  63%|▋| 158/250 [00:41<00:38,  2.38it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  64%|▋| 159/250 [00:41<00:32,  2.83it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  64%|▋| 161/250 [00:41<00:24,  3.59it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  65%|▋| 162/250 [00:42<00:44,  1.96it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  65%|▋| 163/250 [00:43<00:38,  2.26it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  66%|▋| 164/250 [00:43<00:39,  2.17it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  66%|▋| 165/250 [00:43<00:32,  2.64it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  66%|▋| 166/250 [00:44<00:37,  2.22it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  67%|▋| 167/250 [00:45<00:40,  2.06it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  67%|▋| 168/250 [00:45<00:30,  2.65it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  68%|▋| 169/250 [00:45<00:31,  2.53it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  68%|▋| 170/250 [00:46<00:32,  2.43it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  69%|▋| 172/250 [00:46<00:22,  3.51it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  69%|▋| 173/250 [00:46<00:28,  2.67it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  70%|▋| 174/250 [00:47<00:39,  1.90it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  70%|▋| 176/250 [00:48<00:26,  2.75it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  71%|▋| 177/250 [00:49<00:46,  1.56it/s, est. speed input: 28\u001b[A\n",
      "Processed prompts:  72%|▋| 179/250 [00:50<00:38,  1.84it/s, est. speed input: 28\u001b[A\n",
      "Processed prompts:  72%|▋| 181/250 [00:51<00:32,  2.09it/s, est. speed input: 28\u001b[A\n",
      "Processed prompts:  73%|▋| 182/250 [00:52<00:44,  1.55it/s, est. speed input: 28\u001b[A\n",
      "Processed prompts:  73%|▋| 183/250 [00:53<00:48,  1.38it/s, est. speed input: 27\u001b[A\n",
      "Processed prompts:  74%|▋| 184/250 [00:54<00:46,  1.42it/s, est. speed input: 27\u001b[A\n",
      "Processed prompts:  74%|▋| 185/250 [00:55<00:51,  1.27it/s, est. speed input: 27\u001b[A\n",
      "Processed prompts:  74%|▋| 186/250 [00:56<00:54,  1.18it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  75%|▋| 187/250 [00:56<00:43,  1.44it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  75%|▊| 188/250 [00:57<00:47,  1.30it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  76%|▊| 189/250 [00:58<00:50,  1.21it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  76%|▊| 190/250 [00:59<00:46,  1.30it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  76%|▊| 191/250 [00:59<00:41,  1.43it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  77%|▊| 192/250 [00:59<00:30,  1.90it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  77%|▊| 193/250 [01:00<00:33,  1.68it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  78%|▊| 194/250 [01:01<00:37,  1.51it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  78%|▊| 195/250 [01:03<01:03,  1.15s/it, est. speed input: 25\u001b[A\n",
      "Processed prompts:  78%|▊| 196/250 [01:04<00:52,  1.03it/s, est. speed input: 25\u001b[A\n",
      "Processed prompts:  79%|▊| 198/250 [01:05<00:46,  1.12it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  80%|▊| 199/250 [01:06<00:37,  1.37it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  80%|▊| 200/250 [01:06<00:35,  1.41it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  80%|▊| 201/250 [01:06<00:29,  1.66it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  81%|▊| 202/250 [01:07<00:25,  1.85it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  81%|▊| 203/250 [01:08<00:39,  1.18it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  82%|▊| 204/250 [01:09<00:38,  1.18it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  82%|▊| 206/250 [01:10<00:24,  1.80it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  83%|▊| 207/250 [01:12<00:45,  1.06s/it, est. speed input: 23\u001b[A\n",
      "Processed prompts:  83%|▊| 208/250 [01:16<01:17,  1.84s/it, est. speed input: 22\u001b[A\n",
      "Processed prompts:  84%|▊| 211/250 [01:17<00:35,  1.09it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  85%|▊| 212/250 [01:17<00:30,  1.25it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  85%|▊| 213/250 [01:17<00:25,  1.43it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  86%|▊| 214/250 [01:18<00:23,  1.52it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  86%|▊| 215/250 [01:19<00:29,  1.18it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  86%|▊| 216/250 [01:22<00:49,  1.44s/it, est. speed input: 21\u001b[A\n",
      "Processed prompts:  87%|▊| 217/250 [01:23<00:42,  1.28s/it, est. speed input: 21\u001b[A\n",
      "Processed prompts:  87%|▊| 218/250 [01:25<00:48,  1.51s/it, est. speed input: 21\u001b[A\n",
      "Processed prompts:  88%|▉| 219/250 [01:26<00:35,  1.15s/it, est. speed input: 21\u001b[A\n",
      "Processed prompts:  88%|▉| 220/250 [01:26<00:26,  1.11it/s, est. speed input: 21\u001b[A\n",
      "Processed prompts:  89%|▉| 222/250 [01:29<00:33,  1.18s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts:  89%|▉| 223/250 [01:29<00:24,  1.09it/s, est. speed input: 20\u001b[A\n",
      "Processed prompts:  90%|▉| 224/250 [01:30<00:23,  1.10it/s, est. speed input: 20\u001b[A\n",
      "Processed prompts:  90%|▉| 225/250 [01:33<00:36,  1.47s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts:  90%|▉| 226/250 [01:33<00:27,  1.15s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts:  91%|▉| 227/250 [01:33<00:19,  1.16it/s, est. speed input: 20\u001b[A\n",
      "Processed prompts:  91%|▉| 228/250 [01:35<00:23,  1.05s/it, est. speed input: 19\u001b[A\n",
      "Processed prompts:  92%|▉| 230/250 [01:36<00:17,  1.14it/s, est. speed input: 19\u001b[A\n",
      "Processed prompts:  92%|▉| 231/250 [01:37<00:14,  1.33it/s, est. speed input: 19\u001b[A\n",
      "Processed prompts:  93%|▉| 232/250 [01:43<00:41,  2.28s/it, est. speed input: 18\u001b[A\n",
      "Processed prompts:  93%|▉| 233/250 [01:43<00:28,  1.70s/it, est. speed input: 18\u001b[A\n",
      "Processed prompts:  94%|▉| 235/250 [01:47<00:27,  1.85s/it, est. speed input: 18\u001b[A\n",
      "Processed prompts:  94%|▉| 236/250 [01:50<00:26,  1.92s/it, est. speed input: 17\u001b[A\n",
      "Processed prompts:  95%|▉| 237/250 [01:51<00:25,  1.93s/it, est. speed input: 17\u001b[A\n",
      "Processed prompts:  95%|▉| 238/250 [01:54<00:26,  2.18s/it, est. speed input: 17\u001b[A\n",
      "Processed prompts:  96%|▉| 239/250 [01:55<00:17,  1.63s/it, est. speed input: 17\u001b[A\n",
      "Processed prompts:  96%|▉| 240/250 [02:01<00:29,  2.91s/it, est. speed input: 16\u001b[A\n",
      "Processed prompts:  96%|▉| 241/250 [02:01<00:19,  2.14s/it, est. speed input: 16\u001b[A\n",
      "Processed prompts:  97%|▉| 242/250 [02:01<00:12,  1.57s/it, est. speed input: 16\u001b[A\n",
      "Processed prompts:  97%|▉| 243/250 [02:04<00:14,  2.00s/it, est. speed input: 16\u001b[A\n",
      "Processed prompts:  98%|▉| 244/250 [02:08<00:15,  2.58s/it, est. speed input: 15\u001b[A\n",
      "Processed prompts:  98%|▉| 245/250 [02:15<00:19,  3.81s/it, est. speed input: 15\u001b[A\n",
      "Processed prompts:  98%|▉| 246/250 [02:19<00:15,  3.83s/it, est. speed input: 14\u001b[A\n",
      "Processed prompts:  99%|▉| 247/250 [02:34<00:21,  7.28s/it, est. speed input: 13\u001b[A\n",
      "Processed prompts:  99%|▉| 248/250 [02:35<00:10,  5.31s/it, est. speed input: 13\u001b[A\n",
      "Processed prompts: 100%|▉| 249/250 [02:40<00:05,  5.33s/it, est. speed input: 13\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [03:02<00:00, 10.37s/it, est. speed input: 11\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [03:02<00:00,  1.37it/s, est. speed input: 11\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [03:02<00:00,  1.37it/s]\n",
      "2026-01-08:15:08:47 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-08:15:08:47 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: None, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_en|      3|flexible-extract|     0|exact_match|↑  |0.856|±  |0.0222|\n",
      "|              |       |strict-match    |     0|exact_match|↑  |0.004|±  |0.0040|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_en\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_en \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a82b538-9474-4069-8a0b-6f58b89cf7e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08:15:19:53 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-08:15:20:20 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-08:15:21:07 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_bn']\n",
      "2026-01-08:15:21:07 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-08:15:21:07 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-08:15:21:07 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:15\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:16\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:16\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:16\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:17\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:18\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:57399 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:18\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:19\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:19\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:20\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.88it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.88it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:20\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.59 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:21\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.415361 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:26\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/f195ca22f3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:26\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 4.38 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:35\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 7.038 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:35\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 11.42 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:36\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:36\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:36\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 37.75it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:39\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1831)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:39\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 18.33 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:21:40\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "bn/train-00000-of-00001.parquet: 100%|█████| 8.90k/8.90k [00:00<00:00, 13.3kB/s]\n",
      "bn/test-00000-of-00001.parquet: 100%|███████| 63.7k/63.7k [00:00<00:00, 232kB/s]\n",
      "Generating train split: 100%|████████████| 8/8 [00:00<00:00, 2215.40 examples/s]\n",
      "Generating test split: 100%|███████| 250/250 [00:00<00:00, 113469.97 examples/s]\n",
      "2026-01-08:15:21:46 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-08:15:21:46 INFO     [tasks:691] Task: mgsm_en_cot_bn (mgsm/en_cot/mgsm_en_cot_bn.yaml)\n",
      "2026-01-08:15:21:46 INFO     [evaluator:313] mgsm_en_cot_bn: Using gen_kwargs: {'do_sample': True, 'until': ['প্রশ্ন:', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-08:15:21:46 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-08:15:21:46 INFO     [api.task:310] Building contexts for mgsm_en_cot_bn on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 496.10it/s]\n",
      "2026-01-08:15:21:47 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████| 250/250 [00:00<00:00, 8640.07it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:02<11:55,  2.87s/it, est. speed input: 106.\u001b[A\n",
      "Processed prompts:   1%| | 2/250 [00:05<11:57,  2.89s/it, est. speed input: 75.2\u001b[A\n",
      "Processed prompts:   1%| | 3/250 [00:05<06:46,  1.65s/it, est. speed input: 105.\u001b[A\n",
      "Processed prompts:   2%| | 4/250 [00:06<05:47,  1.41s/it, est. speed input: 126.\u001b[A\n",
      "Processed prompts:   2%| | 5/250 [00:07<04:56,  1.21s/it, est. speed input: 130.\u001b[A\n",
      "Processed prompts:   2%| | 6/250 [00:07<03:26,  1.18it/s, est. speed input: 161.\u001b[A\n",
      "Processed prompts:   3%| | 8/250 [00:08<01:50,  2.20it/s, est. speed input: 208.\u001b[A\n",
      "Processed prompts:   4%| | 10/250 [00:08<01:09,  3.47it/s, est. speed input: 243\u001b[A\n",
      "Processed prompts:   5%| | 12/250 [00:08<00:57,  4.17it/s, est. speed input: 279\u001b[A\n",
      "Processed prompts:   6%| | 14/250 [00:08<00:42,  5.62it/s, est. speed input: 316\u001b[A\n",
      "Processed prompts:   6%| | 16/250 [00:08<00:34,  6.81it/s, est. speed input: 346\u001b[A\n",
      "Processed prompts:   7%| | 18/250 [00:09<00:31,  7.39it/s, est. speed input: 382\u001b[A\n",
      "Processed prompts:   8%| | 21/250 [00:09<00:25,  9.10it/s, est. speed input: 459\u001b[A\n",
      "Processed prompts:   9%| | 23/250 [00:09<00:21, 10.43it/s, est. speed input: 498\u001b[A\n",
      "Processed prompts:  10%| | 25/250 [00:09<00:23,  9.76it/s, est. speed input: 549\u001b[A\n",
      "Processed prompts:  11%| | 28/250 [00:09<00:21, 10.33it/s, est. speed input: 608\u001b[A\n",
      "Processed prompts:  13%|▏| 33/250 [00:09<00:13, 16.13it/s, est. speed input: 737\u001b[A\n",
      "Processed prompts:  14%|▏| 36/250 [00:10<00:15, 13.96it/s, est. speed input: 803\u001b[A\n",
      "Processed prompts:  15%|▏| 38/250 [00:10<00:17, 11.87it/s, est. speed input: 833\u001b[A\n",
      "Processed prompts:  17%|▏| 42/250 [00:10<00:13, 15.82it/s, est. speed input: 906\u001b[A\n",
      "Processed prompts:  18%|▏| 45/250 [00:10<00:11, 17.83it/s, est. speed input: 958\u001b[A\n",
      "Processed prompts:  19%|▏| 48/250 [00:11<00:17, 11.29it/s, est. speed input: 980\u001b[A\n",
      "Processed prompts:  21%|▏| 53/250 [00:11<00:12, 15.76it/s, est. speed input: 108\u001b[A\n",
      "Processed prompts:  22%|▏| 56/250 [00:11<00:11, 16.42it/s, est. speed input: 112\u001b[A\n",
      "Processed prompts:  24%|▏| 59/250 [00:11<00:11, 16.49it/s, est. speed input: 117\u001b[A\n",
      "Processed prompts:  25%|▎| 63/250 [00:11<00:10, 17.58it/s, est. speed input: 123\u001b[A\n",
      "Processed prompts:  26%|▎| 66/250 [00:12<00:10, 18.05it/s, est. speed input: 127\u001b[A\n",
      "Processed prompts:  29%|▎| 72/250 [00:12<00:07, 25.42it/s, est. speed input: 141\u001b[A\n",
      "Processed prompts:  30%|▎| 76/250 [00:12<00:07, 23.63it/s, est. speed input: 149\u001b[A\n",
      "Processed prompts:  32%|▎| 79/250 [00:12<00:08, 19.38it/s, est. speed input: 151\u001b[A\n",
      "Processed prompts:  33%|▎| 83/250 [00:12<00:07, 22.77it/s, est. speed input: 157\u001b[A\n",
      "Processed prompts:  34%|▎| 86/250 [00:12<00:07, 22.26it/s, est. speed input: 165\u001b[A\n",
      "Processed prompts:  36%|▎| 89/250 [00:13<00:07, 20.94it/s, est. speed input: 171\u001b[A\n",
      "Processed prompts:  37%|▎| 92/250 [00:13<00:07, 20.81it/s, est. speed input: 176\u001b[A\n",
      "Processed prompts:  38%|▍| 95/250 [00:13<00:08, 18.65it/s, est. speed input: 179\u001b[A\n",
      "Processed prompts:  39%|▍| 98/250 [00:13<00:07, 20.91it/s, est. speed input: 183\u001b[A\n",
      "Processed prompts:  40%|▍| 101/250 [00:13<00:06, 21.99it/s, est. speed input: 18\u001b[A\n",
      "Processed prompts:  42%|▍| 105/250 [00:13<00:05, 25.12it/s, est. speed input: 19\u001b[A\n",
      "Processed prompts:  44%|▍| 109/250 [00:13<00:06, 23.24it/s, est. speed input: 20\u001b[A\n",
      "Processed prompts:  45%|▍| 112/250 [00:14<00:06, 21.99it/s, est. speed input: 20\u001b[A\n",
      "Processed prompts:  46%|▍| 115/250 [00:14<00:07, 17.76it/s, est. speed input: 20\u001b[A\n",
      "Processed prompts:  47%|▍| 118/250 [00:14<00:07, 16.62it/s, est. speed input: 21\u001b[A\n",
      "Processed prompts:  48%|▍| 121/250 [00:14<00:08, 14.90it/s, est. speed input: 21\u001b[A\n",
      "Processed prompts:  49%|▍| 123/250 [00:15<00:09, 13.77it/s, est. speed input: 21\u001b[A\n",
      "Processed prompts:  50%|▌| 125/250 [00:15<00:10, 11.64it/s, est. speed input: 21\u001b[A\n",
      "Processed prompts:  51%|▌| 127/250 [00:15<00:10, 11.48it/s, est. speed input: 21\u001b[A\n",
      "Processed prompts:  52%|▌| 129/250 [00:15<00:11, 10.78it/s, est. speed input: 21\u001b[A\n",
      "Processed prompts:  53%|▌| 133/250 [00:15<00:07, 15.59it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  54%|▌| 136/250 [00:15<00:06, 16.57it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  55%|▌| 138/250 [00:16<00:06, 17.03it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  57%|▌| 142/250 [00:16<00:09, 11.75it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  58%|▌| 145/250 [00:16<00:07, 13.26it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  59%|▌| 148/250 [00:16<00:07, 14.21it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  60%|▌| 151/250 [00:16<00:06, 16.24it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  61%|▌| 153/250 [00:17<00:06, 14.87it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  62%|▌| 156/250 [00:17<00:05, 15.93it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  63%|▋| 158/250 [00:17<00:06, 14.32it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  64%|▋| 160/250 [00:17<00:08, 10.33it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  65%|▋| 162/250 [00:18<00:09,  9.62it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  66%|▋| 164/250 [00:18<00:09,  9.00it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  66%|▋| 166/250 [00:18<00:10,  8.11it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  67%|▋| 168/250 [00:19<00:11,  7.21it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  68%|▋| 171/250 [00:19<00:09,  8.06it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  69%|▋| 172/250 [00:19<00:10,  7.62it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  70%|▋| 175/250 [00:19<00:09,  7.56it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  70%|▋| 176/250 [00:20<00:13,  5.39it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  72%|▋| 179/250 [00:20<00:13,  5.40it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  73%|▋| 182/250 [00:21<00:09,  6.89it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  73%|▋| 183/250 [00:21<00:11,  5.79it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  74%|▋| 184/250 [00:21<00:13,  5.00it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  74%|▋| 185/250 [00:22<00:17,  3.62it/s, est. speed input: 21\u001b[A\n",
      "Processed prompts:  74%|▋| 186/250 [00:23<00:23,  2.73it/s, est. speed input: 21\u001b[A\n",
      "Processed prompts:  75%|▋| 187/250 [00:23<00:20,  3.05it/s, est. speed input: 21\u001b[A\n",
      "Processed prompts:  75%|▊| 188/250 [00:23<00:23,  2.68it/s, est. speed input: 20\u001b[A\n",
      "Processed prompts:  76%|▊| 189/250 [00:24<00:27,  2.19it/s, est. speed input: 20\u001b[A\n",
      "Processed prompts:  76%|▊| 190/250 [00:25<00:31,  1.92it/s, est. speed input: 19\u001b[A\n",
      "Processed prompts:  76%|▊| 191/250 [00:26<00:39,  1.50it/s, est. speed input: 19\u001b[A\n",
      "Processed prompts:  77%|▊| 193/250 [00:26<00:25,  2.25it/s, est. speed input: 19\u001b[A\n",
      "Processed prompts:  78%|▊| 195/250 [00:28<00:32,  1.71it/s, est. speed input: 18\u001b[A\n",
      "Processed prompts:  78%|▊| 196/250 [00:28<00:28,  1.90it/s, est. speed input: 18\u001b[A\n",
      "Processed prompts:  79%|▊| 197/250 [00:29<00:37,  1.42it/s, est. speed input: 17\u001b[A\n",
      "Processed prompts:  79%|▊| 198/250 [00:29<00:29,  1.75it/s, est. speed input: 17\u001b[A\n",
      "Processed prompts:  80%|▊| 199/250 [00:31<00:43,  1.17it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  80%|▊| 200/250 [00:32<00:38,  1.30it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  80%|▊| 201/250 [00:32<00:34,  1.42it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  81%|▊| 202/250 [00:34<00:51,  1.08s/it, est. speed input: 15\u001b[A\n",
      "Processed prompts:  81%|▊| 203/250 [00:35<00:43,  1.07it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  82%|▊| 204/250 [00:36<00:53,  1.15s/it, est. speed input: 14\u001b[A\n",
      "Processed prompts:  82%|▊| 205/250 [00:37<00:41,  1.08it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  82%|▊| 206/250 [00:38<00:45,  1.03s/it, est. speed input: 14\u001b[A\n",
      "Processed prompts:  83%|▊| 207/250 [00:38<00:35,  1.20it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  83%|▊| 208/250 [00:39<00:34,  1.21it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  84%|▊| 209/250 [00:39<00:25,  1.60it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  84%|▊| 210/250 [00:40<00:28,  1.41it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  84%|▊| 211/250 [00:42<00:34,  1.13it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  85%|▊| 212/250 [00:49<01:45,  2.78s/it, est. speed input: 11\u001b[A\n",
      "Processed prompts:  85%|▊| 213/250 [00:56<02:36,  4.22s/it, est. speed input: 10\u001b[A\n",
      "Processed prompts:  86%|▊| 214/250 [00:58<02:02,  3.40s/it, est. speed input: 10\u001b[A\n",
      "Processed prompts:  86%|▊| 215/250 [00:58<01:27,  2.50s/it, est. speed input: 10\u001b[A\n",
      "Processed prompts:  86%|▊| 216/250 [01:01<01:24,  2.47s/it, est. speed input: 97\u001b[A\n",
      "Processed prompts:  87%|▊| 217/250 [01:02<01:07,  2.03s/it, est. speed input: 96\u001b[A\n",
      "Processed prompts:  87%|▊| 218/250 [01:10<02:08,  4.01s/it, est. speed input: 85\u001b[A\n",
      "Processed prompts:  88%|▉| 219/250 [01:12<01:42,  3.30s/it, est. speed input: 83\u001b[A\n",
      "Processed prompts:  88%|▉| 220/250 [01:17<01:58,  3.93s/it, est. speed input: 78\u001b[A\n",
      "Processed prompts:  88%|▉| 221/250 [01:17<01:20,  2.79s/it, est. speed input: 79\u001b[A\n",
      "Processed prompts:  89%|▉| 222/250 [01:18<01:00,  2.15s/it, est. speed input: 79\u001b[A\n",
      "Processed prompts:  89%|▉| 223/250 [01:19<00:45,  1.67s/it, est. speed input: 78\u001b[A\n",
      "Processed prompts:  90%|▉| 224/250 [01:22<00:58,  2.25s/it, est. speed input: 75\u001b[A\n",
      "Processed prompts:  90%|▉| 225/250 [01:29<01:27,  3.49s/it, est. speed input: 70\u001b[A\n",
      "Processed prompts:  90%|▉| 226/250 [01:38<02:06,  5.25s/it, est. speed input: 64\u001b[A\n",
      "Processed prompts:  91%|▉| 227/250 [01:40<01:37,  4.23s/it, est. speed input: 63\u001b[A\n",
      "Processed prompts:  91%|▉| 228/250 [01:48<01:55,  5.27s/it, est. speed input: 59\u001b[A\n",
      "Processed prompts:  92%|▉| 229/250 [02:03<02:55,  8.36s/it, est. speed input: 52\u001b[A\n",
      "Processed prompts:  92%|▉| 230/250 [02:05<02:09,  6.45s/it, est. speed input: 51\u001b[A\n",
      "Processed prompts:  92%|▉| 231/250 [02:09<01:47,  5.65s/it, est. speed input: 50\u001b[A\n",
      "Processed prompts:  93%|▉| 232/250 [02:13<01:35,  5.31s/it, est. speed input: 48\u001b[A\n",
      "Processed prompts:  93%|▉| 233/250 [02:16<01:14,  4.39s/it, est. speed input: 48\u001b[A\n",
      "Processed prompts:  94%|▉| 234/250 [02:19<01:07,  4.20s/it, est. speed input: 47\u001b[A\n",
      "Processed prompts:  94%|▉| 235/250 [02:33<01:44,  6.99s/it, est. speed input: 43\u001b[A\n",
      "Processed prompts:  94%|▉| 236/250 [02:33<01:10,  5.04s/it, est. speed input: 43\u001b[A\n",
      "Processed prompts:  95%|▉| 237/250 [02:34<00:49,  3.77s/it, est. speed input: 43\u001b[A\n",
      "Processed prompts:  95%|▉| 238/250 [02:36<00:39,  3.29s/it, est. speed input: 43\u001b[A\n",
      "Processed prompts:  96%|▉| 239/250 [03:10<02:16, 12.41s/it, est. speed input: 35\u001b[A\n",
      "Processed prompts:  96%|▉| 240/250 [03:14<01:39,  9.92s/it, est. speed input: 35\u001b[A\n",
      "Processed prompts:  96%|▉| 241/250 [03:22<01:24,  9.42s/it, est. speed input: 33\u001b[A\n",
      "Processed prompts:  97%|▉| 242/250 [03:53<02:05, 15.63s/it, est. speed input: 29\u001b[A\n",
      "Processed prompts:  97%|▉| 243/250 [04:17<02:08, 18.35s/it, est. speed input: 27\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [04:17<00:00,  1.03s/it, est. speed input: 27\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [04:17<00:00,  1.03s/it]\n",
      "2026-01-08:15:26:07 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-08:15:26:07 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: None, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_bn|      3|flexible-extract|     0|exact_match|↑  |0.192|±  | 0.025|\n",
      "|              |       |strict-match    |     0|exact_match|↑  |0.000|±  | 0.000|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_bn\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_bn \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0dab824-159e-4a3c-a7a0-5d50dae0304a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08:15:28:11 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-08:15:28:36 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-08:15:29:18 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_de']\n",
      "2026-01-08:15:29:18 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-08:15:29:18 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-08:15:29:18 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:24\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:24\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:24\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:25\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:26\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:32\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:52827 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:32\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:35\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:35\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:36\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.80it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.80it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:37\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.62 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:37\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.469688 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:43\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/f195ca22f3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:43\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 5.48 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:47\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.222 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:47\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 6.71 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:47\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:48\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:48\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 38.89it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:51\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2236)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:51\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 13.55 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:29:51\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "de/train-00000-of-00001.parquet: 100%|█████| 5.92k/5.92k [00:00<00:00, 8.97kB/s]\n",
      "de/test-00000-of-00001.parquet: 100%|███████| 44.1k/44.1k [00:00<00:00, 129kB/s]\n",
      "Generating train split: 100%|████████████| 8/8 [00:00<00:00, 1144.30 examples/s]\n",
      "Generating test split: 100%|████████| 250/250 [00:00<00:00, 99797.85 examples/s]\n",
      "2026-01-08:15:29:57 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-08:15:29:57 INFO     [tasks:691] Task: mgsm_en_cot_de (mgsm/en_cot/mgsm_en_cot_de.yaml)\n",
      "2026-01-08:15:29:57 INFO     [evaluator:313] mgsm_en_cot_de: Using gen_kwargs: {'do_sample': True, 'until': ['Frage:', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-08:15:29:57 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-08:15:29:57 INFO     [api.task:310] Building contexts for mgsm_en_cot_de on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 557.05it/s]\n",
      "2026-01-08:15:29:58 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|█████████████████████| 250/250 [00:00<00:00, 11014.34it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:04<17:33,  4.23s/it, est. speed input: 13.4\u001b[A\n",
      "Processed prompts:   1%| | 2/250 [00:04<08:48,  2.13s/it, est. speed input: 24.3\u001b[A\n",
      "Processed prompts:   1%| | 3/250 [00:05<05:54,  1.44s/it, est. speed input: 36.3\u001b[A\n",
      "Processed prompts:   2%| | 4/250 [00:05<03:48,  1.07it/s, est. speed input: 53.2\u001b[A\n",
      "Processed prompts:   2%| | 5/250 [00:06<03:04,  1.33it/s, est. speed input: 68.4\u001b[A\n",
      "Processed prompts:   2%| | 6/250 [00:06<02:10,  1.87it/s, est. speed input: 76.7\u001b[A\n",
      "Processed prompts:   3%| | 7/250 [00:06<01:38,  2.48it/s, est. speed input: 92.1\u001b[A\n",
      "Processed prompts:   3%| | 8/250 [00:06<01:17,  3.13it/s, est. speed input: 99.9\u001b[A\n",
      "Processed prompts:   4%| | 9/250 [00:06<01:05,  3.70it/s, est. speed input: 108.\u001b[A\n",
      "Processed prompts:   4%| | 10/250 [00:06<00:59,  4.00it/s, est. speed input: 114\u001b[A\n",
      "Processed prompts:   4%| | 11/250 [00:07<00:59,  4.00it/s, est. speed input: 122\u001b[A\n",
      "Processed prompts:   5%| | 12/250 [00:07<00:51,  4.64it/s, est. speed input: 130\u001b[A\n",
      "Processed prompts:   6%| | 14/250 [00:07<00:37,  6.36it/s, est. speed input: 145\u001b[A\n",
      "Processed prompts:   7%| | 17/250 [00:07<00:22, 10.26it/s, est. speed input: 172\u001b[A\n",
      "Processed prompts:   8%| | 19/250 [00:07<00:21, 10.98it/s, est. speed input: 191\u001b[A\n",
      "Processed prompts:  10%| | 24/250 [00:07<00:13, 16.95it/s, est. speed input: 245\u001b[A\n",
      "Processed prompts:  10%| | 26/250 [00:08<00:15, 14.62it/s, est. speed input: 263\u001b[A\n",
      "Processed prompts:  11%| | 28/250 [00:08<00:15, 14.71it/s, est. speed input: 289\u001b[A\n",
      "Processed prompts:  12%| | 30/250 [00:08<00:14, 14.81it/s, est. speed input: 311\u001b[A\n",
      "Processed prompts:  13%|▏| 32/250 [00:08<00:15, 14.16it/s, est. speed input: 323\u001b[A\n",
      "Processed prompts:  15%|▏| 37/250 [00:08<00:12, 17.41it/s, est. speed input: 359\u001b[A\n",
      "Processed prompts:  16%|▏| 39/250 [00:08<00:14, 14.10it/s, est. speed input: 376\u001b[A\n",
      "Processed prompts:  18%|▏| 44/250 [00:09<00:10, 20.25it/s, est. speed input: 423\u001b[A\n",
      "Processed prompts:  19%|▏| 47/250 [00:09<00:09, 21.95it/s, est. speed input: 444\u001b[A\n",
      "Processed prompts:  20%|▏| 50/250 [00:09<00:10, 19.66it/s, est. speed input: 466\u001b[A\n",
      "Processed prompts:  21%|▏| 53/250 [00:09<00:10, 18.35it/s, est. speed input: 485\u001b[A\n",
      "Processed prompts:  23%|▏| 57/250 [00:09<00:08, 22.28it/s, est. speed input: 526\u001b[A\n",
      "Processed prompts:  26%|▎| 65/250 [00:09<00:05, 34.72it/s, est. speed input: 596\u001b[A\n",
      "Processed prompts:  28%|▎| 70/250 [00:09<00:04, 37.94it/s, est. speed input: 647\u001b[A\n",
      "Processed prompts:  30%|▎| 75/250 [00:10<00:05, 31.09it/s, est. speed input: 678\u001b[A\n",
      "Processed prompts:  32%|▎| 79/250 [00:10<00:05, 32.84it/s, est. speed input: 710\u001b[A\n",
      "Processed prompts:  34%|▎| 86/250 [00:10<00:04, 39.65it/s, est. speed input: 784\u001b[A\n",
      "Processed prompts:  36%|▎| 91/250 [00:10<00:05, 29.66it/s, est. speed input: 812\u001b[A\n",
      "Processed prompts:  38%|▍| 95/250 [00:10<00:05, 27.80it/s, est. speed input: 833\u001b[A\n",
      "Processed prompts:  40%|▍| 99/250 [00:10<00:05, 28.21it/s, est. speed input: 857\u001b[A\n",
      "Processed prompts:  42%|▍| 106/250 [00:10<00:04, 35.72it/s, est. speed input: 91\u001b[A\n",
      "Processed prompts:  44%|▍| 111/250 [00:11<00:04, 29.97it/s, est. speed input: 94\u001b[A\n",
      "Processed prompts:  46%|▍| 115/250 [00:11<00:04, 30.49it/s, est. speed input: 97\u001b[A\n",
      "Processed prompts:  48%|▍| 119/250 [00:11<00:04, 26.40it/s, est. speed input: 99\u001b[A\n",
      "Processed prompts:  50%|▌| 125/250 [00:11<00:03, 31.89it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  52%|▌| 129/250 [00:11<00:04, 27.05it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  54%|▌| 135/250 [00:12<00:03, 31.68it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  56%|▌| 141/250 [00:12<00:03, 29.19it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  59%|▌| 147/250 [00:12<00:03, 32.85it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  60%|▌| 151/250 [00:12<00:03, 31.68it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  63%|▋| 157/250 [00:12<00:02, 35.04it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  64%|▋| 161/250 [00:12<00:03, 25.25it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  66%|▋| 164/250 [00:13<00:03, 25.83it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  67%|▋| 167/250 [00:13<00:03, 25.77it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  69%|▋| 172/250 [00:13<00:02, 29.31it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  72%|▋| 179/250 [00:13<00:01, 37.74it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  74%|▋| 184/250 [00:13<00:01, 38.89it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  76%|▊| 189/250 [00:13<00:01, 37.95it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  77%|▊| 193/250 [00:13<00:01, 37.34it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  79%|▊| 197/250 [00:14<00:01, 28.73it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  80%|▊| 201/250 [00:14<00:02, 24.19it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  82%|▊| 204/250 [00:14<00:02, 22.02it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  83%|▊| 207/250 [00:14<00:02, 18.45it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  84%|▊| 211/250 [00:14<00:01, 20.70it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  86%|▊| 214/250 [00:14<00:01, 22.18it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  87%|▊| 217/250 [00:15<00:02, 15.23it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  88%|▉| 220/250 [00:15<00:01, 17.43it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  89%|▉| 223/250 [00:15<00:02, 12.09it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  90%|▉| 225/250 [00:15<00:01, 12.86it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  91%|▉| 227/250 [00:16<00:03,  5.87it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  92%|▉| 229/250 [00:17<00:03,  5.81it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  92%|▉| 231/250 [00:18<00:06,  2.86it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  93%|▉| 232/250 [00:19<00:06,  2.62it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  93%|▉| 233/250 [00:21<00:12,  1.39it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  94%|▉| 234/250 [00:21<00:09,  1.64it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  94%|▉| 235/250 [00:22<00:10,  1.43it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  94%|▉| 236/250 [00:23<00:08,  1.59it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  95%|▉| 237/250 [00:23<00:06,  1.87it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  95%|▉| 238/250 [00:25<00:12,  1.04s/it, est. speed input: 97\u001b[A\n",
      "Processed prompts:  96%|▉| 239/250 [00:26<00:09,  1.16it/s, est. speed input: 96\u001b[A\n",
      "Processed prompts:  96%|▉| 240/250 [00:29<00:13,  1.39s/it, est. speed input: 87\u001b[A\n",
      "Processed prompts:  96%|▉| 241/250 [00:31<00:13,  1.54s/it, est. speed input: 82\u001b[A\n",
      "Processed prompts:  97%|▉| 242/250 [00:47<00:46,  5.78s/it, est. speed input: 54\u001b[A\n",
      "Processed prompts:  97%|▉| 243/250 [00:47<00:30,  4.30s/it, est. speed input: 53\u001b[A\n",
      "Processed prompts:  98%|▉| 244/250 [01:33<01:39, 16.50s/it, est. speed input: 27\u001b[A\n",
      "Processed prompts:  98%|▉| 245/250 [02:06<01:46, 21.39s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts:  98%|▉| 246/250 [02:38<01:38, 24.64s/it, est. speed input: 16\u001b[A\n",
      "Processed prompts:  99%|▉| 247/250 [03:00<01:11, 23.70s/it, est. speed input: 14\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [03:00<00:00,  1.39it/s, est. speed input: 14\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [03:00<00:00,  1.39it/s]\n",
      "2026-01-08:15:33:01 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-08:15:33:01 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: None, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_de|      3|flexible-extract|     0|exact_match|↑  |0.396|±  | 0.031|\n",
      "|              |       |strict-match    |     0|exact_match|↑  |0.000|±  | 0.000|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_de\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_de \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c1882e8-9239-4519-89c2-cdbab62f8fc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08:15:34:33 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-08:15:34:46 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-08:15:35:12 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_es']\n",
      "2026-01-08:15:35:12 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-08:15:35:12 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-08:15:35:12 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:18\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:19\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:19\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:19\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:20\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:21\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:45179 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:21\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:22\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:22\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:23\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.84it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.84it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:23\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.60 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:24\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.424984 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:30\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/f195ca22f3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:30\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 5.14 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:33\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.128 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:33\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 6.26 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:34\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:34\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:34\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 38.17it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:37\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2612)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:37\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 13.03 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:35:37\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "es/train-00000-of-00001.parquet: 100%|█████| 5.72k/5.72k [00:00<00:00, 8.29kB/s]\n",
      "es/test-00000-of-00001.parquet: 100%|███████| 42.5k/42.5k [00:00<00:00, 149kB/s]\n",
      "Generating train split: 100%|████████████| 8/8 [00:00<00:00, 2238.16 examples/s]\n",
      "Generating test split: 100%|███████| 250/250 [00:00<00:00, 137464.08 examples/s]\n",
      "2026-01-08:15:35:44 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-08:15:35:44 INFO     [tasks:691] Task: mgsm_en_cot_es (mgsm/en_cot/mgsm_en_cot_es.yaml)\n",
      "2026-01-08:15:35:44 INFO     [evaluator:313] mgsm_en_cot_es: Using gen_kwargs: {'do_sample': True, 'until': ['Pregunta:', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-08:15:35:44 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-08:15:35:44 INFO     [api.task:310] Building contexts for mgsm_en_cot_es on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 498.02it/s]\n",
      "2026-01-08:15:35:44 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████| 250/250 [00:00<00:00, 9169.88it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:02<10:17,  2.48s/it, est. speed input: 22.5\u001b[A\n",
      "Processed prompts:   1%| | 2/250 [00:02<04:30,  1.09s/it, est. speed input: 60.7\u001b[A\n",
      "Processed prompts:   1%| | 3/250 [00:02<02:40,  1.54it/s, est. speed input: 87.4\u001b[A\n",
      "Processed prompts:   2%| | 5/250 [00:02<01:21,  3.01it/s, est. speed input: 132.\u001b[A\n",
      "Processed prompts:   2%| | 6/250 [00:03<01:09,  3.53it/s, est. speed input: 148.\u001b[A\n",
      "Processed prompts:   4%| | 9/250 [00:03<00:39,  6.11it/s, est. speed input: 210.\u001b[A\n",
      "Processed prompts:   5%| | 13/250 [00:03<00:26,  9.02it/s, est. speed input: 281\u001b[A\n",
      "Processed prompts:   6%| | 15/250 [00:03<00:23, 10.21it/s, est. speed input: 324\u001b[A\n",
      "Processed prompts:   7%| | 17/250 [00:03<00:19, 11.76it/s, est. speed input: 373\u001b[A\n",
      "Processed prompts:   8%| | 19/250 [00:04<00:28,  8.01it/s, est. speed input: 383\u001b[A\n",
      "Processed prompts:   8%| | 21/250 [00:04<00:29,  7.68it/s, est. speed input: 401\u001b[A\n",
      "Processed prompts:   9%| | 23/250 [00:05<00:43,  5.22it/s, est. speed input: 381\u001b[A\n",
      "Processed prompts:  10%| | 24/250 [00:05<00:40,  5.59it/s, est. speed input: 387\u001b[A\n",
      "Processed prompts:  10%| | 26/250 [00:05<00:30,  7.31it/s, est. speed input: 427\u001b[A\n",
      "Processed prompts:  11%| | 28/250 [00:06<00:48,  4.62it/s, est. speed input: 397\u001b[A\n",
      "Processed prompts:  12%| | 29/250 [00:06<00:44,  4.93it/s, est. speed input: 405\u001b[A\n",
      "Processed prompts:  12%| | 30/250 [00:06<00:40,  5.37it/s, est. speed input: 407\u001b[A\n",
      "Processed prompts:  12%| | 31/250 [00:06<00:36,  5.97it/s, est. speed input: 413\u001b[A\n",
      "Processed prompts:  14%|▏| 34/250 [00:06<00:25,  8.56it/s, est. speed input: 437\u001b[A\n",
      "Processed prompts:  14%|▏| 36/250 [00:07<00:27,  7.66it/s, est. speed input: 444\u001b[A\n",
      "Processed prompts:  16%|▏| 40/250 [00:07<00:19, 10.66it/s, est. speed input: 470\u001b[A\n",
      "Processed prompts:  17%|▏| 43/250 [00:07<00:15, 13.08it/s, est. speed input: 488\u001b[A\n",
      "Processed prompts:  18%|▏| 45/250 [00:07<00:18, 11.33it/s, est. speed input: 498\u001b[A\n",
      "Processed prompts:  20%|▏| 51/250 [00:07<00:10, 18.53it/s, est. speed input: 568\u001b[A\n",
      "Processed prompts:  23%|▏| 57/250 [00:07<00:07, 25.03it/s, est. speed input: 618\u001b[A\n",
      "Processed prompts:  24%|▏| 61/250 [00:08<00:09, 19.28it/s, est. speed input: 640\u001b[A\n",
      "Processed prompts:  26%|▎| 64/250 [00:08<00:09, 19.72it/s, est. speed input: 660\u001b[A\n",
      "Processed prompts:  28%|▎| 71/250 [00:08<00:06, 28.43it/s, est. speed input: 721\u001b[A\n",
      "Processed prompts:  30%|▎| 75/250 [00:08<00:05, 29.83it/s, est. speed input: 750\u001b[A\n",
      "Processed prompts:  32%|▎| 79/250 [00:08<00:05, 29.49it/s, est. speed input: 784\u001b[A\n",
      "Processed prompts:  33%|▎| 83/250 [00:08<00:05, 29.26it/s, est. speed input: 812\u001b[A\n",
      "Processed prompts:  35%|▎| 88/250 [00:08<00:04, 32.79it/s, est. speed input: 847\u001b[A\n",
      "Processed prompts:  38%|▍| 95/250 [00:09<00:03, 38.89it/s, est. speed input: 899\u001b[A\n",
      "Processed prompts:  40%|▍| 100/250 [00:09<00:03, 38.64it/s, est. speed input: 94\u001b[A\n",
      "Processed prompts:  42%|▍| 106/250 [00:09<00:03, 36.62it/s, est. speed input: 97\u001b[A\n",
      "Processed prompts:  46%|▍| 114/250 [00:09<00:02, 45.81it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  48%|▍| 119/250 [00:09<00:02, 46.56it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  51%|▌| 127/250 [00:09<00:02, 53.04it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  53%|▌| 133/250 [00:10<00:03, 32.03it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  55%|▌| 138/250 [00:10<00:03, 33.16it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  57%|▌| 143/250 [00:10<00:04, 26.70it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  59%|▌| 147/250 [00:10<00:03, 27.19it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  61%|▌| 153/250 [00:10<00:03, 29.63it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  65%|▋| 162/250 [00:10<00:02, 40.76it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  67%|▋| 167/250 [00:11<00:02, 39.14it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  69%|▋| 172/250 [00:11<00:01, 40.52it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  71%|▋| 177/250 [00:11<00:02, 35.51it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  73%|▋| 182/250 [00:11<00:01, 37.17it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  76%|▊| 189/250 [00:11<00:01, 43.27it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  78%|▊| 194/250 [00:11<00:01, 37.51it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  80%|▊| 199/250 [00:12<00:02, 23.39it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  82%|▊| 205/250 [00:12<00:01, 27.02it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  84%|▊| 210/250 [00:12<00:01, 29.94it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  86%|▊| 214/250 [00:12<00:01, 22.35it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  87%|▊| 217/250 [00:12<00:01, 23.50it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  88%|▉| 220/250 [00:13<00:01, 18.38it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  89%|▉| 223/250 [00:13<00:01, 15.02it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  90%|▉| 225/250 [00:13<00:01, 15.00it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  92%|▉| 229/250 [00:13<00:01, 14.14it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  92%|▉| 231/250 [00:14<00:01, 13.39it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  93%|▉| 233/250 [00:14<00:01, 10.84it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  94%|▉| 235/250 [00:14<00:01, 10.88it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  95%|▉| 238/250 [00:14<00:00, 13.35it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  96%|▉| 240/250 [00:15<00:01,  8.44it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  97%|▉| 242/250 [00:15<00:00,  8.16it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  98%|▉| 244/250 [00:16<00:01,  4.21it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  98%|▉| 245/250 [00:17<00:01,  3.79it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  98%|▉| 246/250 [00:25<00:07,  1.78s/it, est. speed input: 99\u001b[A\n",
      "Processed prompts:  99%|▉| 247/250 [00:54<00:22,  7.63s/it, est. speed input: 46\u001b[A\n",
      "Processed prompts:  99%|▉| 248/250 [00:55<00:12,  6.01s/it, est. speed input: 45\u001b[A\n",
      "Processed prompts: 100%|▉| 249/250 [02:29<00:27, 27.89s/it, est. speed input: 17\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [02:29<00:00,  1.67it/s, est. speed input: 17\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [02:29<00:00,  1.67it/s]\n",
      "2026-01-08:15:38:16 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-08:15:38:16 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: None, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_es|      3|flexible-extract|     0|exact_match|↑  |0.484|±  |0.0317|\n",
      "|              |       |strict-match    |     0|exact_match|↑  |0.000|±  |0.0000|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_es\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_es \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14185071-1bca-4ed7-895c-ffcd4c0438d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08:15:38:59 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-08:15:39:14 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-08:15:39:50 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_fr']\n",
      "2026-01-08:15:39:51 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-08:15:39:51 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-08:15:39:51 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:39:56\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:39:57\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:39:57\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:39:57\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:39:58\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:39:59\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:49791 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:39:59\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:00\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:01\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:01\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.89it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.89it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:02\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.59 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:02\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.484584 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:07\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/f195ca22f3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:07\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 4.26 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:10\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.158 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:10\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 5.42 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:11\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:11\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:11\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 35.29it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:14\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2958)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:14\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 11.64 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:40:15\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "fr/train-00000-of-00001.parquet: 100%|█████| 5.76k/5.76k [00:00<00:00, 9.68kB/s]\n",
      "fr/test-00000-of-00001.parquet: 100%|███████| 44.3k/44.3k [00:00<00:00, 182kB/s]\n",
      "Generating train split: 100%|████████████| 8/8 [00:00<00:00, 1847.10 examples/s]\n",
      "Generating test split: 100%|███████| 250/250 [00:00<00:00, 108807.31 examples/s]\n",
      "2026-01-08:15:40:21 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-08:15:40:21 INFO     [tasks:691] Task: mgsm_en_cot_fr (mgsm/en_cot/mgsm_en_cot_fr.yaml)\n",
      "2026-01-08:15:40:21 INFO     [evaluator:313] mgsm_en_cot_fr: Using gen_kwargs: {'do_sample': True, 'until': ['Question :', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-08:15:40:21 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-08:15:40:21 INFO     [api.task:310] Building contexts for mgsm_en_cot_fr on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 494.71it/s]\n",
      "2026-01-08:15:40:21 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████| 250/250 [00:00<00:00, 8742.65it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:03<16:34,  3.99s/it, est. speed input: 19.7\u001b[A\n",
      "Processed prompts:   1%| | 2/250 [00:05<09:37,  2.33s/it, est. speed input: 33.7\u001b[A\n",
      "Processed prompts:   1%| | 3/250 [00:05<05:34,  1.36s/it, est. speed input: 43.5\u001b[A\n",
      "Processed prompts:   2%| | 4/250 [00:05<04:10,  1.02s/it, est. speed input: 52.9\u001b[A\n",
      "Processed prompts:   2%| | 6/250 [00:06<02:08,  1.90it/s, est. speed input: 77.7\u001b[A\n",
      "Processed prompts:   3%| | 8/250 [00:06<01:21,  2.97it/s, est. speed input: 100.\u001b[A\n",
      "Processed prompts:   4%| | 10/250 [00:06<00:55,  4.36it/s, est. speed input: 125\u001b[A\n",
      "Processed prompts:   5%| | 12/250 [00:06<00:45,  5.23it/s, est. speed input: 144\u001b[A\n",
      "Processed prompts:   6%| | 16/250 [00:06<00:26,  8.71it/s, est. speed input: 187\u001b[A\n",
      "Processed prompts:   8%| | 19/250 [00:06<00:20, 11.43it/s, est. speed input: 222\u001b[A\n",
      "Processed prompts:   8%| | 21/250 [00:06<00:18, 12.62it/s, est. speed input: 239\u001b[A\n",
      "Processed prompts:   9%| | 23/250 [00:07<00:24,  9.24it/s, est. speed input: 246\u001b[A\n",
      "Processed prompts:  11%| | 27/250 [00:07<00:16, 13.15it/s, est. speed input: 286\u001b[A\n",
      "Processed prompts:  12%| | 30/250 [00:07<00:17, 12.52it/s, est. speed input: 301\u001b[A\n",
      "Processed prompts:  13%|▏| 33/250 [00:07<00:15, 13.65it/s, est. speed input: 326\u001b[A\n",
      "Processed prompts:  14%|▏| 36/250 [00:08<00:16, 12.90it/s, est. speed input: 343\u001b[A\n",
      "Processed prompts:  16%|▏| 40/250 [00:08<00:12, 16.97it/s, est. speed input: 376\u001b[A\n",
      "Processed prompts:  18%|▏| 44/250 [00:08<00:09, 20.81it/s, est. speed input: 417\u001b[A\n",
      "Processed prompts:  19%|▏| 47/250 [00:08<00:11, 17.15it/s, est. speed input: 437\u001b[A\n",
      "Processed prompts:  20%|▏| 50/250 [00:08<00:12, 16.20it/s, est. speed input: 454\u001b[A\n",
      "Processed prompts:  22%|▏| 54/250 [00:09<00:10, 18.21it/s, est. speed input: 496\u001b[A\n",
      "Processed prompts:  23%|▏| 58/250 [00:09<00:08, 21.94it/s, est. speed input: 526\u001b[A\n",
      "Processed prompts:  25%|▏| 62/250 [00:09<00:08, 21.57it/s, est. speed input: 556\u001b[A\n",
      "Processed prompts:  26%|▎| 65/250 [00:09<00:08, 23.02it/s, est. speed input: 579\u001b[A\n",
      "Processed prompts:  27%|▎| 68/250 [00:09<00:07, 24.35it/s, est. speed input: 601\u001b[A\n",
      "Processed prompts:  30%|▎| 74/250 [00:09<00:05, 32.55it/s, est. speed input: 653\u001b[A\n",
      "Processed prompts:  31%|▎| 78/250 [00:09<00:05, 34.14it/s, est. speed input: 684\u001b[A\n",
      "Processed prompts:  33%|▎| 83/250 [00:09<00:05, 33.13it/s, est. speed input: 722\u001b[A\n",
      "Processed prompts:  37%|▎| 92/250 [00:09<00:03, 45.53it/s, est. speed input: 819\u001b[A\n",
      "Processed prompts:  39%|▍| 97/250 [00:10<00:03, 43.14it/s, est. speed input: 851\u001b[A\n",
      "Processed prompts:  41%|▍| 102/250 [00:10<00:04, 32.00it/s, est. speed input: 87\u001b[A\n",
      "Processed prompts:  44%|▍| 109/250 [00:10<00:04, 35.13it/s, est. speed input: 93\u001b[A\n",
      "Processed prompts:  45%|▍| 113/250 [00:10<00:03, 34.35it/s, est. speed input: 95\u001b[A\n",
      "Processed prompts:  47%|▍| 117/250 [00:10<00:03, 34.91it/s, est. speed input: 98\u001b[A\n",
      "Processed prompts:  48%|▍| 121/250 [00:10<00:04, 31.81it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  50%|▌| 125/250 [00:11<00:03, 33.50it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  52%|▌| 129/250 [00:11<00:04, 24.81it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  55%|▌| 137/250 [00:11<00:03, 32.01it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  56%|▌| 141/250 [00:11<00:03, 33.05it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  59%|▌| 147/250 [00:11<00:02, 38.38it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  61%|▌| 152/250 [00:11<00:03, 31.75it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  62%|▌| 156/250 [00:12<00:03, 28.68it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  65%|▋| 162/250 [00:12<00:02, 33.60it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  67%|▋| 167/250 [00:12<00:02, 35.86it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  68%|▋| 171/250 [00:12<00:02, 34.75it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  70%|▋| 175/250 [00:12<00:02, 33.16it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  72%|▋| 179/250 [00:12<00:02, 32.20it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  74%|▋| 185/250 [00:12<00:01, 32.55it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  76%|▊| 189/250 [00:13<00:01, 34.08it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  77%|▊| 193/250 [00:13<00:01, 31.78it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  79%|▊| 197/250 [00:13<00:01, 27.11it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  80%|▊| 200/250 [00:13<00:02, 22.89it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  81%|▊| 203/250 [00:13<00:02, 16.87it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  82%|▊| 206/250 [00:14<00:02, 16.81it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  83%|▊| 208/250 [00:14<00:02, 17.30it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  85%|▊| 212/250 [00:14<00:01, 20.88it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  87%|▊| 217/250 [00:14<00:01, 19.12it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  88%|▉| 221/250 [00:14<00:01, 20.94it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  90%|▉| 224/250 [00:15<00:01, 15.83it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  90%|▉| 226/250 [00:15<00:01, 15.95it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  92%|▉| 229/250 [00:15<00:01, 13.69it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  92%|▉| 231/250 [00:16<00:02,  7.74it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  93%|▉| 233/250 [00:16<00:02,  7.18it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  94%|▉| 235/250 [00:17<00:03,  4.11it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  94%|▉| 236/250 [00:17<00:03,  4.03it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  95%|▉| 237/250 [00:18<00:03,  4.25it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  96%|▉| 239/250 [00:18<00:02,  4.51it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  96%|▉| 240/250 [00:20<00:06,  1.56it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  96%|▉| 241/250 [00:21<00:05,  1.61it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  97%|▉| 242/250 [00:21<00:04,  1.99it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  97%|▉| 243/250 [00:21<00:03,  2.02it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  98%|▉| 244/250 [00:23<00:05,  1.16it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  98%|▉| 245/250 [00:24<00:03,  1.27it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  98%|▉| 246/250 [00:27<00:06,  1.52s/it, est. speed input: 92\u001b[A\n",
      "Processed prompts:  99%|▉| 247/250 [00:32<00:07,  2.35s/it, est. speed input: 80\u001b[A\n",
      "Processed prompts:  99%|▉| 248/250 [02:08<00:59, 29.68s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts: 100%|▉| 249/250 [02:34<00:28, 28.74s/it, est. speed input: 16\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [02:34<00:00,  1.61it/s, est. speed input: 16\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [02:35<00:00,  1.61it/s]\n",
      "2026-01-08:15:42:58 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-08:15:42:58 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: None, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_fr|      3|flexible-extract|     0|exact_match|↑  |  0.5|±  |0.0317|\n",
      "|              |       |strict-match    |     0|exact_match|↑  |  0.0|±  |0.0000|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_fr\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_fr \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f172a84-5285-4e6b-96fe-95348c0ef953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08:15:44:18 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-08:15:44:43 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-08:15:45:20 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_ja']\n",
      "2026-01-08:15:45:20 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-08:15:45:20 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-08:15:45:20 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:25\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:26\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:26\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:27\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:29\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:29\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:57669 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:29\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:30\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:30\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:31\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:31\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.57 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:32\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.341194 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:38\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/f195ca22f3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:38\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 4.93 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:41\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.221 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:41\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 6.15 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:41\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:42\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:42\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 38.62it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:45\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3323)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:45\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 12.99 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:45:45\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "ja/train-00000-of-00001.parquet: 100%|█████| 6.44k/6.44k [00:00<00:00, 9.42kB/s]\n",
      "ja/test-00000-of-00001.parquet: 100%|███████| 47.0k/47.0k [00:00<00:00, 171kB/s]\n",
      "Generating train split: 100%|████████████| 8/8 [00:00<00:00, 1155.77 examples/s]\n",
      "Generating test split: 100%|████████| 250/250 [00:00<00:00, 91698.82 examples/s]\n",
      "2026-01-08:15:45:52 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-08:15:45:52 INFO     [tasks:691] Task: mgsm_en_cot_ja (mgsm/en_cot/mgsm_en_cot_ja.yaml)\n",
      "2026-01-08:15:45:52 INFO     [evaluator:313] mgsm_en_cot_ja: Using gen_kwargs: {'do_sample': True, 'until': ['問題：', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-08:15:45:52 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-08:15:45:52 INFO     [api.task:310] Building contexts for mgsm_en_cot_ja on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 476.90it/s]\n",
      "2026-01-08:15:45:52 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████| 250/250 [00:00<00:00, 9025.75it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:02<10:56,  2.64s/it, est. speed input: 32.9\u001b[A\n",
      "Processed prompts:   1%| | 2/250 [00:02<05:19,  1.29s/it, est. speed input: 53.2\u001b[A\n",
      "Processed prompts:   1%| | 3/250 [00:03<04:36,  1.12s/it, est. speed input: 60.5\u001b[A\n",
      "Processed prompts:   2%| | 4/250 [00:04<03:01,  1.36it/s, est. speed input: 74.5\u001b[A\n",
      "Processed prompts:   2%| | 5/250 [00:04<02:04,  1.97it/s, est. speed input: 87.1\u001b[A\n",
      "Processed prompts:   2%| | 6/250 [00:04<01:32,  2.64it/s, est. speed input: 108.\u001b[A\n",
      "Processed prompts:   3%| | 7/250 [00:04<01:27,  2.78it/s, est. speed input: 122.\u001b[A\n",
      "Processed prompts:   4%| | 9/250 [00:04<00:54,  4.40it/s, est. speed input: 151.\u001b[A\n",
      "Processed prompts:   4%| | 11/250 [00:04<00:39,  6.03it/s, est. speed input: 192\u001b[A\n",
      "Processed prompts:   5%| | 12/250 [00:05<00:43,  5.49it/s, est. speed input: 202\u001b[A\n",
      "Processed prompts:   6%| | 15/250 [00:05<00:26,  8.85it/s, est. speed input: 236\u001b[A\n",
      "Processed prompts:   7%| | 18/250 [00:05<00:18, 12.29it/s, est. speed input: 292\u001b[A\n",
      "Processed prompts:   8%| | 20/250 [00:05<00:17, 13.09it/s, est. speed input: 319\u001b[A\n",
      "Processed prompts:   9%| | 22/250 [00:05<00:17, 12.72it/s, est. speed input: 334\u001b[A\n",
      "Processed prompts:  10%| | 24/250 [00:05<00:19, 11.59it/s, est. speed input: 352\u001b[A\n",
      "Processed prompts:  11%| | 27/250 [00:06<00:14, 15.02it/s, est. speed input: 382\u001b[A\n",
      "Processed prompts:  12%| | 31/250 [00:06<00:11, 19.36it/s, est. speed input: 425\u001b[A\n",
      "Processed prompts:  14%|▏| 36/250 [00:06<00:08, 26.05it/s, est. speed input: 490\u001b[A\n",
      "Processed prompts:  16%|▏| 40/250 [00:06<00:08, 26.19it/s, est. speed input: 543\u001b[A\n",
      "Processed prompts:  17%|▏| 43/250 [00:06<00:11, 18.14it/s, est. speed input: 554\u001b[A\n",
      "Processed prompts:  18%|▏| 46/250 [00:06<00:11, 17.64it/s, est. speed input: 580\u001b[A\n",
      "Processed prompts:  20%|▏| 49/250 [00:07<00:13, 15.25it/s, est. speed input: 598\u001b[A\n",
      "Processed prompts:  21%|▏| 52/250 [00:07<00:12, 16.47it/s, est. speed input: 624\u001b[A\n",
      "Processed prompts:  22%|▏| 54/250 [00:07<00:11, 17.08it/s, est. speed input: 637\u001b[A\n",
      "Processed prompts:  23%|▏| 58/250 [00:07<00:09, 20.16it/s, est. speed input: 677\u001b[A\n",
      "Processed prompts:  25%|▏| 62/250 [00:07<00:08, 23.42it/s, est. speed input: 711\u001b[A\n",
      "Processed prompts:  27%|▎| 67/250 [00:07<00:08, 21.69it/s, est. speed input: 751\u001b[A\n",
      "Processed prompts:  29%|▎| 72/250 [00:08<00:06, 26.18it/s, est. speed input: 811\u001b[A\n",
      "Processed prompts:  31%|▎| 77/250 [00:08<00:05, 29.93it/s, est. speed input: 865\u001b[A\n",
      "Processed prompts:  32%|▎| 81/250 [00:08<00:06, 24.70it/s, est. speed input: 883\u001b[A\n",
      "Processed prompts:  34%|▎| 86/250 [00:08<00:05, 28.80it/s, est. speed input: 922\u001b[A\n",
      "Processed prompts:  36%|▎| 91/250 [00:08<00:05, 31.28it/s, est. speed input: 967\u001b[A\n",
      "Processed prompts:  38%|▍| 95/250 [00:08<00:04, 32.44it/s, est. speed input: 991\u001b[A\n",
      "Processed prompts:  40%|▍| 99/250 [00:08<00:04, 32.04it/s, est. speed input: 102\u001b[A\n",
      "Processed prompts:  41%|▍| 103/250 [00:09<00:04, 30.51it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  43%|▍| 107/250 [00:09<00:04, 32.26it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  44%|▍| 111/250 [00:09<00:04, 32.28it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  46%|▍| 116/250 [00:09<00:03, 35.67it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  48%|▍| 121/250 [00:09<00:03, 38.94it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  51%|▌| 127/250 [00:09<00:02, 42.94it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  53%|▌| 132/250 [00:09<00:03, 33.57it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  54%|▌| 136/250 [00:09<00:03, 30.38it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  56%|▌| 141/250 [00:10<00:03, 33.12it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  58%|▌| 145/250 [00:10<00:03, 26.25it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  59%|▌| 148/250 [00:10<00:04, 25.20it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  61%|▌| 153/250 [00:10<00:03, 29.95it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  63%|▋| 158/250 [00:10<00:03, 25.59it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  64%|▋| 161/250 [00:10<00:03, 26.01it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  66%|▋| 164/250 [00:11<00:03, 23.56it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  67%|▋| 167/250 [00:11<00:04, 17.86it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  68%|▋| 171/250 [00:11<00:03, 20.35it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  70%|▋| 175/250 [00:11<00:03, 23.08it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  71%|▋| 178/250 [00:11<00:03, 20.01it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  72%|▋| 181/250 [00:11<00:03, 21.38it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  74%|▋| 184/250 [00:12<00:02, 22.78it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  76%|▊| 189/250 [00:12<00:02, 24.64it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  77%|▊| 192/250 [00:12<00:03, 16.55it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  78%|▊| 195/250 [00:12<00:02, 18.37it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  79%|▊| 198/250 [00:13<00:03, 15.18it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  80%|▊| 200/250 [00:13<00:03, 13.24it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  81%|▊| 202/250 [00:13<00:04, 11.79it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  82%|▊| 205/250 [00:13<00:03, 14.12it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  83%|▊| 208/250 [00:13<00:02, 15.06it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  84%|▊| 210/250 [00:14<00:07,  5.70it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  85%|▊| 212/250 [00:15<00:08,  4.40it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  85%|▊| 213/250 [00:15<00:08,  4.49it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  86%|▊| 214/250 [00:16<00:08,  4.41it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  86%|▊| 215/250 [00:16<00:09,  3.60it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  87%|▊| 217/250 [00:16<00:06,  4.77it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  88%|▉| 219/250 [00:16<00:05,  6.15it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  88%|▉| 220/250 [00:25<00:56,  1.88s/it, est. speed input: 88\u001b[A\n",
      "Processed prompts:  88%|▉| 221/250 [00:36<01:48,  3.74s/it, est. speed input: 63\u001b[A\n",
      "Processed prompts:  89%|▉| 222/250 [00:41<01:54,  4.08s/it, est. speed input: 55\u001b[A\n",
      "Processed prompts:  89%|▉| 223/250 [00:42<01:29,  3.31s/it, est. speed input: 54\u001b[A\n",
      "Processed prompts:  90%|▉| 224/250 [00:56<02:42,  6.25s/it, est. speed input: 40\u001b[A\n",
      "Processed prompts:  90%|▉| 225/250 [01:00<02:17,  5.49s/it, est. speed input: 38\u001b[A\n",
      "Processed prompts:  90%|▉| 226/250 [01:02<01:53,  4.72s/it, est. speed input: 37\u001b[A\n",
      "Processed prompts:  91%|▉| 227/250 [01:11<02:17,  5.97s/it, est. speed input: 32\u001b[A\n",
      "Processed prompts:  91%|▉| 228/250 [01:20<02:28,  6.76s/it, est. speed input: 29\u001b[A\n",
      "Processed prompts:  92%|▉| 229/250 [01:23<01:55,  5.48s/it, est. speed input: 28\u001b[A\n",
      "Processed prompts:  92%|▉| 230/250 [01:50<03:58, 11.91s/it, est. speed input: 21\u001b[A\n",
      "Processed prompts:  92%|▉| 231/250 [01:57<03:17, 10.39s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts:  93%|▉| 232/250 [02:17<04:01, 13.43s/it, est. speed input: 17\u001b[A\n",
      "Processed prompts:  93%|▉| 233/250 [02:49<05:20, 18.84s/it, est. speed input: 14\u001b[A\n",
      "Processed prompts:  94%|▉| 234/250 [02:53<03:53, 14.58s/it, est. speed input: 14\u001b[A\n",
      "Processed prompts:  94%|▉| 235/250 [03:52<06:56, 27.78s/it, est. speed input: 10\u001b[A\n",
      "Processed prompts:  94%|▉| 236/250 [04:05<05:28, 23.43s/it, est. speed input: 10\u001b[A\n",
      "Processed prompts:  95%|▉| 237/250 [04:29<05:07, 23.63s/it, est. speed input: 92\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [04:29<00:00,  1.08s/it, est. speed input: 98\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [04:29<00:00,  1.08s/it]\n",
      "2026-01-08:15:50:27 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-08:15:50:27 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: None, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_ja|      3|flexible-extract|     0|exact_match|↑  |0.296|±  |0.0289|\n",
      "|              |       |strict-match    |     0|exact_match|↑  |0.000|±  |0.0000|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_ja\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_ja \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95307aa4-2cec-465f-a542-a94dde323df3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08:15:51:31 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-08:15:51:44 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-08:15:52:14 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_ru']\n",
      "2026-01-08:15:52:14 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-08:15:52:14 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-08:15:52:14 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:19\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:19\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:19\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:19\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:21\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:22\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:53293 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:22\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:25\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:27\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:28\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:28\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.54 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:29\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 3.525733 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:34\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/f195ca22f3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:34\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 5.17 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:38\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.083 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:38\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 6.25 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:38\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:38\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:38\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:01<00:00, 34.36it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:41\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3699)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:41\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 12.69 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:52:42\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "ru/train-00000-of-00001.parquet: 100%|█████| 7.49k/7.49k [00:00<00:00, 12.4kB/s]\n",
      "ru/test-00000-of-00001.parquet: 100%|███████| 57.6k/57.6k [00:00<00:00, 194kB/s]\n",
      "Generating train split: 100%|████████████| 8/8 [00:00<00:00, 2218.91 examples/s]\n",
      "Generating test split: 100%|███████| 250/250 [00:00<00:00, 127331.63 examples/s]\n",
      "2026-01-08:15:52:49 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-08:15:52:49 INFO     [tasks:691] Task: mgsm_en_cot_ru (mgsm/en_cot/mgsm_en_cot_ru.yaml)\n",
      "2026-01-08:15:52:49 INFO     [evaluator:313] mgsm_en_cot_ru: Using gen_kwargs: {'do_sample': True, 'until': ['Задача:', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-08:15:52:49 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-08:15:52:49 INFO     [api.task:310] Building contexts for mgsm_en_cot_ru on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 487.32it/s]\n",
      "2026-01-08:15:52:50 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████| 250/250 [00:00<00:00, 8855.47it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:02<09:14,  2.23s/it, est. speed input: 43.1\u001b[A\n",
      "Processed prompts:   1%| | 2/250 [00:02<04:05,  1.01it/s, est. speed input: 80.9\u001b[A\n",
      "Processed prompts:   1%| | 3/250 [00:02<02:51,  1.44it/s, est. speed input: 99.1\u001b[A\n",
      "Processed prompts:   2%| | 4/250 [00:03<02:15,  1.82it/s, est. speed input: 111.\u001b[A\n",
      "Processed prompts:   3%| | 7/250 [00:03<00:54,  4.42it/s, est. speed input: 183.\u001b[A\n",
      "Processed prompts:   5%| | 13/250 [00:03<00:22, 10.63it/s, est. speed input: 338\u001b[A\n",
      "Processed prompts:   7%| | 18/250 [00:03<00:18, 12.64it/s, est. speed input: 431\u001b[A\n",
      "Processed prompts:   8%| | 21/250 [00:03<00:17, 13.13it/s, est. speed input: 530\u001b[A\n",
      "Processed prompts:   9%| | 23/250 [00:03<00:17, 13.35it/s, est. speed input: 564\u001b[A\n",
      "Processed prompts:  11%| | 27/250 [00:04<00:15, 14.79it/s, est. speed input: 645\u001b[A\n",
      "Processed prompts:  13%|▏| 32/250 [00:04<00:11, 18.71it/s, est. speed input: 727\u001b[A\n",
      "Processed prompts:  14%|▏| 35/250 [00:04<00:10, 20.66it/s, est. speed input: 768\u001b[A\n",
      "Processed prompts:  15%|▏| 38/250 [00:04<00:10, 20.10it/s, est. speed input: 808\u001b[A\n",
      "Processed prompts:  16%|▏| 41/250 [00:04<00:10, 19.10it/s, est. speed input: 847\u001b[A\n",
      "Processed prompts:  18%|▏| 44/250 [00:04<00:12, 16.64it/s, est. speed input: 869\u001b[A\n",
      "Processed prompts:  18%|▏| 46/250 [00:05<00:14, 14.19it/s, est. speed input: 888\u001b[A\n",
      "Processed prompts:  20%|▏| 50/250 [00:05<00:11, 17.03it/s, est. speed input: 965\u001b[A\n",
      "Processed prompts:  21%|▏| 52/250 [00:05<00:13, 14.36it/s, est. speed input: 967\u001b[A\n",
      "Processed prompts:  22%|▏| 55/250 [00:05<00:11, 16.56it/s, est. speed input: 100\u001b[A\n",
      "Processed prompts:  23%|▏| 57/250 [00:05<00:15, 12.07it/s, est. speed input: 981\u001b[A\n",
      "Processed prompts:  24%|▏| 60/250 [00:06<00:13, 14.18it/s, est. speed input: 101\u001b[A\n",
      "Processed prompts:  25%|▎| 63/250 [00:06<00:12, 15.52it/s, est. speed input: 103\u001b[A\n",
      "Processed prompts:  26%|▎| 65/250 [00:06<00:19,  9.34it/s, est. speed input: 987\u001b[A\n",
      "Processed prompts:  27%|▎| 67/250 [00:07<00:21,  8.66it/s, est. speed input: 972\u001b[A\n",
      "Processed prompts:  28%|▎| 69/250 [00:07<00:18,  9.93it/s, est. speed input: 999\u001b[A\n",
      "Processed prompts:  29%|▎| 72/250 [00:07<00:13, 13.04it/s, est. speed input: 104\u001b[A\n",
      "Processed prompts:  30%|▎| 76/250 [00:07<00:10, 17.35it/s, est. speed input: 107\u001b[A\n",
      "Processed prompts:  32%|▎| 79/250 [00:07<00:10, 16.04it/s, est. speed input: 107\u001b[A\n",
      "Processed prompts:  32%|▎| 81/250 [00:07<00:12, 13.28it/s, est. speed input: 106\u001b[A\n",
      "Processed prompts:  33%|▎| 83/250 [00:08<00:14, 11.86it/s, est. speed input: 107\u001b[A\n",
      "Processed prompts:  35%|▎| 88/250 [00:08<00:09, 17.93it/s, est. speed input: 114\u001b[A\n",
      "Processed prompts:  37%|▎| 93/250 [00:08<00:06, 22.72it/s, est. speed input: 119\u001b[A\n",
      "Processed prompts:  38%|▍| 96/250 [00:08<00:07, 20.67it/s, est. speed input: 120\u001b[A\n",
      "Processed prompts:  40%|▍| 99/250 [00:08<00:08, 18.72it/s, est. speed input: 120\u001b[A\n",
      "Processed prompts:  41%|▍| 102/250 [00:08<00:08, 17.45it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  42%|▍| 104/250 [00:09<00:09, 15.57it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  43%|▍| 108/250 [00:09<00:07, 19.10it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  44%|▍| 111/250 [00:09<00:06, 20.99it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  46%|▍| 115/250 [00:09<00:06, 19.98it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  48%|▍| 119/250 [00:09<00:06, 20.87it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  49%|▍| 122/250 [00:09<00:06, 20.94it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  50%|▌| 125/250 [00:09<00:05, 21.12it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  51%|▌| 128/250 [00:10<00:08, 14.97it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  54%|▌| 136/250 [00:10<00:04, 25.26it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  56%|▌| 140/250 [00:10<00:04, 26.60it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  58%|▌| 144/250 [00:10<00:04, 25.74it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  59%|▌| 148/250 [00:11<00:04, 20.72it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  61%|▌| 152/250 [00:11<00:04, 21.13it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  62%|▌| 156/250 [00:11<00:04, 22.96it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  64%|▋| 159/250 [00:11<00:04, 19.05it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  65%|▋| 162/250 [00:11<00:04, 18.69it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  67%|▋| 168/250 [00:12<00:03, 20.87it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  69%|▋| 173/250 [00:12<00:03, 24.79it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  71%|▋| 177/250 [00:12<00:02, 26.59it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  72%|▋| 180/250 [00:12<00:03, 19.08it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  73%|▋| 183/250 [00:12<00:03, 19.03it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  74%|▋| 186/250 [00:12<00:03, 16.21it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  75%|▊| 188/250 [00:13<00:03, 16.75it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  76%|▊| 191/250 [00:13<00:03, 18.37it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  78%|▊| 195/250 [00:13<00:02, 21.75it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  79%|▊| 198/250 [00:13<00:02, 17.38it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  80%|▊| 201/250 [00:13<00:03, 13.15it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  81%|▊| 203/250 [00:14<00:04, 10.10it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  82%|▊| 205/250 [00:14<00:04, 11.18it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  83%|▊| 207/250 [00:14<00:03, 11.88it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  84%|▊| 209/250 [00:14<00:03, 10.87it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  84%|▊| 211/250 [00:14<00:03, 12.40it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  86%|▊| 215/250 [00:15<00:02, 15.04it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  87%|▊| 217/250 [00:15<00:02, 13.63it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  88%|▉| 219/250 [00:15<00:02, 13.27it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  88%|▉| 221/250 [00:15<00:03,  9.45it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  89%|▉| 223/250 [00:16<00:02,  9.02it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  90%|▉| 225/250 [00:16<00:04,  5.61it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  90%|▉| 226/250 [00:17<00:04,  5.35it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  91%|▉| 227/250 [00:17<00:03,  5.80it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  92%|▉| 229/250 [00:18<00:08,  2.34it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  92%|▉| 230/250 [00:20<00:12,  1.62it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  93%|▉| 232/250 [00:22<00:13,  1.38it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  93%|▉| 233/250 [00:23<00:15,  1.11it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  94%|▉| 234/250 [00:24<00:13,  1.16it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  94%|▉| 235/250 [00:25<00:12,  1.17it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  94%|▉| 236/250 [00:25<00:10,  1.33it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  95%|▉| 237/250 [00:26<00:09,  1.35it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  95%|▉| 238/250 [00:33<00:29,  2.47s/it, est. speed input: 84\u001b[A\n",
      "Processed prompts:  96%|▉| 239/250 [00:37<00:31,  2.83s/it, est. speed input: 76\u001b[A\n",
      "Processed prompts:  96%|▉| 240/250 [00:52<01:04,  6.46s/it, est. speed input: 54\u001b[A\n",
      "Processed prompts:  97%|▉| 242/250 [01:04<00:50,  6.27s/it, est. speed input: 44\u001b[A\n",
      "Processed prompts:  97%|▉| 243/250 [01:07<00:38,  5.44s/it, est. speed input: 42\u001b[A\n",
      "Processed prompts:  98%|▉| 244/250 [03:07<03:31, 35.20s/it, est. speed input: 15\u001b[A\n",
      "Processed prompts:  98%|▉| 245/250 [03:09<02:10, 26.04s/it, est. speed input: 15\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [03:09<00:00,  1.32it/s, est. speed input: 15\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [03:09<00:00,  1.32it/s]\n",
      "2026-01-08:15:56:01 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-08:15:56:01 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: None, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_ru|      3|flexible-extract|     0|exact_match|↑  |0.448|±  |0.0315|\n",
      "|              |       |strict-match    |     0|exact_match|↑  |0.000|±  |0.0000|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_ru\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_ru \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "778adb7c-075c-4fcf-96d0-13ba71ea7cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08:15:57:21 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-08:15:57:40 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-08:15:58:26 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_sw']\n",
      "2026-01-08:15:58:26 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-08:15:58:26 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-08:15:58:26 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:32\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:32\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:32\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:32\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:34\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:35\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:34315 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:35\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:35\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:36\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:36\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.87it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.87it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:37\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.60 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:38\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.467504 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:42\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/f195ca22f3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:42\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 4.54 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:46\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.144 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:46\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 5.69 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:46\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:46\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:46\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 39.10it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:49\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4087)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:49\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 11.87 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 15:58:50\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "sw/train-00000-of-00001.parquet: 100%|█████| 5.64k/5.64k [00:00<00:00, 8.43kB/s]\n",
      "sw/test-00000-of-00001.parquet: 100%|███████| 41.4k/41.4k [00:00<00:00, 139kB/s]\n",
      "Generating train split: 100%|████████████| 8/8 [00:00<00:00, 2484.78 examples/s]\n",
      "Generating test split: 100%|███████| 250/250 [00:00<00:00, 150635.83 examples/s]\n",
      "2026-01-08:15:58:56 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-08:15:58:56 INFO     [tasks:691] Task: mgsm_en_cot_sw (mgsm/en_cot/mgsm_en_cot_sw.yaml)\n",
      "2026-01-08:15:58:56 INFO     [evaluator:313] mgsm_en_cot_sw: Using gen_kwargs: {'do_sample': True, 'until': ['Swali:', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-08:15:58:56 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-08:15:58:56 INFO     [api.task:310] Building contexts for mgsm_en_cot_sw on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 590.70it/s]\n",
      "2026-01-08:15:58:57 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|█████████████████████| 250/250 [00:00<00:00, 10883.55it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:01<05:26,  1.31s/it, est. speed input: 94.6\u001b[A\n",
      "Processed prompts:   1%| | 2/250 [00:01<02:28,  1.67it/s, est. speed input: 172.\u001b[A\n",
      "Processed prompts:   5%| | 13/250 [00:01<00:16, 14.31it/s, est. speed input: 105\u001b[A\n",
      "Processed prompts:   7%| | 17/250 [00:01<00:13, 17.52it/s, est. speed input: 129\u001b[A\n",
      "Processed prompts:   8%| | 21/250 [00:01<00:12, 17.75it/s, est. speed input: 141\u001b[A\n",
      "Processed prompts:  10%| | 25/250 [00:02<00:26,  8.61it/s, est. speed input: 111\u001b[A\n",
      "Processed prompts:  11%| | 28/250 [00:04<00:54,  4.04it/s, est. speed input: 746\u001b[A\n",
      "Processed prompts:  12%| | 30/250 [00:05<00:48,  4.55it/s, est. speed input: 769\u001b[A\n",
      "Processed prompts:  13%|▏| 32/250 [00:05<00:41,  5.31it/s, est. speed input: 781\u001b[A\n",
      "Processed prompts:  14%|▏| 34/250 [00:05<00:34,  6.31it/s, est. speed input: 803\u001b[A\n",
      "Processed prompts:  14%|▏| 36/250 [00:05<00:32,  6.69it/s, est. speed input: 831\u001b[A\n",
      "Processed prompts:  16%|▏| 39/250 [00:05<00:23,  9.17it/s, est. speed input: 871\u001b[A\n",
      "Processed prompts:  16%|▏| 41/250 [00:05<00:21,  9.56it/s, est. speed input: 875\u001b[A\n",
      "Processed prompts:  17%|▏| 43/250 [00:06<00:22,  9.12it/s, est. speed input: 874\u001b[A\n",
      "Processed prompts:  18%|▏| 46/250 [00:06<00:17, 11.44it/s, est. speed input: 947\u001b[A\n",
      "Processed prompts:  19%|▏| 48/250 [00:06<00:19, 10.13it/s, est. speed input: 950\u001b[A\n",
      "Processed prompts:  20%|▏| 50/250 [00:06<00:20,  9.56it/s, est. speed input: 949\u001b[A\n",
      "Processed prompts:  21%|▏| 52/250 [00:06<00:21,  9.38it/s, est. speed input: 946\u001b[A\n",
      "Processed prompts:  22%|▏| 54/250 [00:07<00:19, 10.08it/s, est. speed input: 977\u001b[A\n",
      "Processed prompts:  22%|▏| 56/250 [00:07<00:26,  7.23it/s, est. speed input: 958\u001b[A\n",
      "Processed prompts:  24%|▏| 59/250 [00:07<00:18, 10.15it/s, est. speed input: 983\u001b[A\n",
      "Processed prompts:  25%|▏| 62/250 [00:07<00:14, 12.84it/s, est. speed input: 101\u001b[A\n",
      "Processed prompts:  26%|▎| 66/250 [00:07<00:11, 15.99it/s, est. speed input: 105\u001b[A\n",
      "Processed prompts:  28%|▎| 69/250 [00:08<00:12, 14.67it/s, est. speed input: 107\u001b[A\n",
      "Processed prompts:  29%|▎| 73/250 [00:08<00:09, 19.05it/s, est. speed input: 111\u001b[A\n",
      "Processed prompts:  30%|▎| 76/250 [00:08<00:12, 14.38it/s, est. speed input: 111\u001b[A\n",
      "Processed prompts:  33%|▎| 82/250 [00:08<00:09, 17.76it/s, est. speed input: 117\u001b[A\n",
      "Processed prompts:  34%|▎| 86/250 [00:08<00:07, 20.76it/s, est. speed input: 121\u001b[A\n",
      "Processed prompts:  36%|▎| 89/250 [00:09<00:08, 19.26it/s, est. speed input: 122\u001b[A\n",
      "Processed prompts:  37%|▎| 92/250 [00:09<00:07, 19.99it/s, est. speed input: 124\u001b[A\n",
      "Processed prompts:  38%|▍| 95/250 [00:09<00:07, 19.96it/s, est. speed input: 126\u001b[A\n",
      "Processed prompts:  39%|▍| 98/250 [00:09<00:08, 18.56it/s, est. speed input: 128\u001b[A\n",
      "Processed prompts:  40%|▍| 101/250 [00:09<00:07, 20.28it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  42%|▍| 104/250 [00:09<00:07, 20.36it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  43%|▍| 107/250 [00:10<00:06, 21.15it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  44%|▍| 110/250 [00:10<00:07, 18.25it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  45%|▍| 113/250 [00:10<00:08, 16.64it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  46%|▍| 116/250 [00:10<00:08, 16.68it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  47%|▍| 118/250 [00:10<00:07, 17.06it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  48%|▍| 121/250 [00:10<00:06, 19.52it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  51%|▌| 127/250 [00:11<00:05, 21.24it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  52%|▌| 130/250 [00:11<00:06, 17.67it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  53%|▌| 133/250 [00:11<00:06, 18.86it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  55%|▌| 137/250 [00:11<00:05, 20.45it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  56%|▌| 140/250 [00:11<00:05, 19.96it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  58%|▌| 146/250 [00:11<00:04, 25.14it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  60%|▌| 149/250 [00:12<00:05, 19.56it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  61%|▌| 153/250 [00:12<00:04, 22.36it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  63%|▋| 157/250 [00:12<00:03, 23.39it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  64%|▋| 160/250 [00:12<00:03, 22.76it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  65%|▋| 163/250 [00:12<00:04, 20.65it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  67%|▋| 167/250 [00:12<00:03, 24.47it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  68%|▋| 170/250 [00:13<00:03, 25.63it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  69%|▋| 173/250 [00:13<00:06, 11.73it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  70%|▋| 176/250 [00:13<00:06, 11.90it/s, est. speed input: 16\u001b[A\n",
      "Processed prompts:  71%|▋| 178/250 [00:14<00:07,  9.79it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  72%|▋| 181/250 [00:14<00:06, 10.38it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  73%|▋| 183/250 [00:14<00:07,  9.44it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  74%|▋| 186/250 [00:15<00:05, 10.69it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  75%|▊| 188/250 [00:15<00:06,  9.88it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  76%|▊| 190/250 [00:15<00:05, 11.06it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  77%|▊| 193/250 [00:15<00:04, 14.13it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  78%|▊| 195/250 [00:15<00:05, 10.71it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  80%|▊| 200/250 [00:16<00:04, 10.35it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  81%|▊| 203/250 [00:16<00:03, 12.59it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  82%|▊| 206/250 [00:16<00:02, 14.95it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  84%|▊| 210/250 [00:17<00:03, 11.12it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  85%|▊| 213/250 [00:17<00:02, 13.06it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  86%|▊| 215/250 [00:17<00:03,  9.88it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  87%|▊| 218/250 [00:17<00:02, 12.29it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  88%|▉| 220/250 [00:18<00:03,  9.09it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  89%|▉| 222/250 [00:18<00:04,  6.64it/s, est. speed input: 15\u001b[A\n",
      "Processed prompts:  90%|▉| 224/250 [00:19<00:05,  4.90it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  90%|▉| 226/250 [00:19<00:04,  5.34it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  91%|▉| 228/250 [00:19<00:03,  5.95it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  92%|▉| 229/250 [00:19<00:03,  6.24it/s, est. speed input: 14\u001b[A\n",
      "Processed prompts:  92%|▉| 231/250 [00:21<00:06,  2.89it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  93%|▉| 232/250 [00:22<00:07,  2.29it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  93%|▉| 233/250 [00:22<00:06,  2.52it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  94%|▉| 235/250 [00:23<00:05,  2.85it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  94%|▉| 236/250 [00:26<00:13,  1.01it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  95%|▉| 237/250 [00:27<00:13,  1.02s/it, est. speed input: 11\u001b[A\n",
      "Processed prompts:  95%|▉| 238/250 [00:28<00:11,  1.02it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  96%|▉| 239/250 [00:29<00:11,  1.03s/it, est. speed input: 10\u001b[A\n",
      "Processed prompts:  96%|▉| 240/250 [00:29<00:08,  1.25it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  96%|▉| 241/250 [00:33<00:13,  1.48s/it, est. speed input: 94\u001b[A\n",
      "Processed prompts:  97%|▉| 242/250 [00:35<00:13,  1.67s/it, est. speed input: 89\u001b[A\n",
      "Processed prompts:  97%|▉| 243/250 [00:37<00:12,  1.84s/it, est. speed input: 84\u001b[A\n",
      "Processed prompts:  98%|▉| 244/250 [00:38<00:09,  1.51s/it, est. speed input: 83\u001b[A\n",
      "Processed prompts:  98%|▉| 245/250 [00:55<00:30,  6.13s/it, est. speed input: 57\u001b[A\n",
      "Processed prompts:  98%|▉| 246/250 [00:57<00:20,  5.06s/it, est. speed input: 55\u001b[A\n",
      "Processed prompts:  99%|▉| 247/250 [01:07<00:19,  6.37s/it, est. speed input: 47\u001b[A\n",
      "Processed prompts:  99%|▉| 248/250 [02:37<01:02, 31.43s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts: 100%|▉| 249/250 [02:42<00:23, 23.44s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [02:42<00:00,  1.54it/s, est. speed input: 20\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [02:42<00:00,  1.54it/s]\n",
      "2026-01-08:16:01:41 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-08:16:01:41 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: None, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_sw|      3|flexible-extract|     0|exact_match|↑  |0.028|±  |0.0105|\n",
      "|              |       |strict-match    |     0|exact_match|↑  |0.000|±  |0.0000|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_sw\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_sw \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58c4721c-00fb-4c56-9716-88d0c050a871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08:16:02:24 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-08:16:02:52 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-08:16:03:27 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_te']\n",
      "2026-01-08:16:03:27 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-08:16:03:27 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-08:16:03:27 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:33\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:34\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:34\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:34\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:35\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:36\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:53779 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:36\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:37\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:37\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:38\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.75it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.75it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:38\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.63 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:39\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.467114 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:45\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/f195ca22f3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:45\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 5.37 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:48\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.226 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:48\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 6.60 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:49\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:49\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:49\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:01<00:00, 32.88it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:53\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4445)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:53\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 13.68 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:03:53\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "te/train-00000-of-00001.parquet: 100%|█████| 10.2k/10.2k [00:00<00:00, 17.8kB/s]\n",
      "te/test-00000-of-00001.parquet: 100%|███████| 68.0k/68.0k [00:00<00:00, 270kB/s]\n",
      "Generating train split: 100%|████████████| 8/8 [00:00<00:00, 1994.08 examples/s]\n",
      "Generating test split: 100%|███████| 250/250 [00:00<00:00, 112003.42 examples/s]\n",
      "2026-01-08:16:04:00 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-08:16:04:00 INFO     [tasks:691] Task: mgsm_en_cot_te (mgsm/en_cot/mgsm_en_cot_te.yaml)\n",
      "2026-01-08:16:04:00 INFO     [evaluator:313] mgsm_en_cot_te: Using gen_kwargs: {'do_sample': True, 'until': ['ప్రశ్న:', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-08:16:04:00 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-08:16:04:00 INFO     [api.task:310] Building contexts for mgsm_en_cot_te on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 481.53it/s]\n",
      "2026-01-08:16:04:00 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████| 250/250 [00:00<00:00, 8108.45it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:05<22:59,  5.54s/it, est. speed input: 57.5\u001b[A\n",
      "Processed prompts:   1%| | 2/250 [00:05<09:54,  2.40s/it, est. speed input: 107.\u001b[A\n",
      "Processed prompts:   2%| | 4/250 [00:05<03:51,  1.06it/s, est. speed input: 182.\u001b[A\n",
      "Processed prompts:   2%| | 6/250 [00:06<02:08,  1.90it/s, est. speed input: 277.\u001b[A\n",
      "Processed prompts:   4%| | 9/250 [00:06<01:12,  3.31it/s, est. speed input: 386.\u001b[A\n",
      "Processed prompts:   4%| | 11/250 [00:06<00:53,  4.46it/s, est. speed input: 478\u001b[A\n",
      "Processed prompts:   5%| | 13/250 [00:07<01:03,  3.72it/s, est. speed input: 549\u001b[A\n",
      "Processed prompts:   6%| | 14/250 [00:07<00:59,  3.98it/s, est. speed input: 634\u001b[A\n",
      "Processed prompts:   6%| | 15/250 [00:07<00:55,  4.25it/s, est. speed input: 667\u001b[A\n",
      "Processed prompts:   6%| | 16/250 [00:07<00:50,  4.65it/s, est. speed input: 686\u001b[A\n",
      "Processed prompts:   7%| | 17/250 [00:08<01:03,  3.66it/s, est. speed input: 689\u001b[A\n",
      "Processed prompts:   8%| | 19/250 [00:08<00:43,  5.26it/s, est. speed input: 787\u001b[A\n",
      "Processed prompts:   8%| | 20/250 [00:09<01:46,  2.16it/s, est. speed input: 718\u001b[A\n",
      "Processed prompts:   9%| | 22/250 [00:09<01:10,  3.21it/s, est. speed input: 764\u001b[A\n",
      "Processed prompts:  10%| | 24/250 [00:09<00:49,  4.60it/s, est. speed input: 826\u001b[A\n",
      "Processed prompts:  10%| | 26/250 [00:09<00:38,  5.87it/s, est. speed input: 860\u001b[A\n",
      "Processed prompts:  11%| | 28/250 [00:10<00:43,  5.05it/s, est. speed input: 872\u001b[A\n",
      "Processed prompts:  12%| | 30/250 [00:10<00:34,  6.42it/s, est. speed input: 906\u001b[A\n",
      "Processed prompts:  13%|▏| 32/250 [00:10<00:27,  7.82it/s, est. speed input: 943\u001b[A\n",
      "Processed prompts:  14%|▏| 34/250 [00:10<00:24,  8.89it/s, est. speed input: 991\u001b[A\n",
      "Processed prompts:  15%|▏| 37/250 [00:11<00:37,  5.72it/s, est. speed input: 100\u001b[A\n",
      "Processed prompts:  15%|▏| 38/250 [00:11<00:35,  6.00it/s, est. speed input: 102\u001b[A\n",
      "Processed prompts:  17%|▏| 43/250 [00:11<00:18, 10.97it/s, est. speed input: 117\u001b[A\n",
      "Processed prompts:  18%|▏| 45/250 [00:12<00:22,  9.31it/s, est. speed input: 120\u001b[A\n",
      "Processed prompts:  19%|▏| 48/250 [00:12<00:17, 11.73it/s, est. speed input: 128\u001b[A\n",
      "Processed prompts:  20%|▏| 50/250 [00:12<00:17, 11.28it/s, est. speed input: 131\u001b[A\n",
      "Processed prompts:  21%|▏| 52/250 [00:12<00:18, 10.86it/s, est. speed input: 137\u001b[A\n",
      "Processed prompts:  22%|▏| 54/250 [00:13<00:21,  9.25it/s, est. speed input: 141\u001b[A\n",
      "Processed prompts:  23%|▏| 58/250 [00:13<00:15, 12.51it/s, est. speed input: 150\u001b[A\n",
      "Processed prompts:  24%|▏| 60/250 [00:13<00:14, 12.76it/s, est. speed input: 153\u001b[A\n",
      "Processed prompts:  25%|▎| 63/250 [00:14<00:27,  6.79it/s, est. speed input: 153\u001b[A\n",
      "Processed prompts:  26%|▎| 65/250 [00:14<00:23,  7.92it/s, est. speed input: 159\u001b[A\n",
      "Processed prompts:  27%|▎| 67/250 [00:14<00:21,  8.63it/s, est. speed input: 162\u001b[A\n",
      "Processed prompts:  28%|▎| 69/250 [00:14<00:18,  9.89it/s, est. speed input: 166\u001b[A\n",
      "Processed prompts:  30%|▎| 74/250 [00:14<00:13, 12.98it/s, est. speed input: 173\u001b[A\n",
      "Processed prompts:  31%|▎| 78/250 [00:15<00:10, 16.57it/s, est. speed input: 179\u001b[A\n",
      "Processed prompts:  32%|▎| 81/250 [00:15<00:09, 18.39it/s, est. speed input: 183\u001b[A\n",
      "Processed prompts:  34%|▎| 85/250 [00:15<00:09, 16.85it/s, est. speed input: 192\u001b[A\n",
      "Processed prompts:  35%|▎| 87/250 [00:15<00:09, 16.41it/s, est. speed input: 195\u001b[A\n",
      "Processed prompts:  36%|▎| 91/250 [00:15<00:09, 16.37it/s, est. speed input: 202\u001b[A\n",
      "Processed prompts:  37%|▎| 93/250 [00:16<00:09, 15.95it/s, est. speed input: 207\u001b[A\n",
      "Processed prompts:  39%|▍| 98/250 [00:16<00:08, 18.90it/s, est. speed input: 216\u001b[A\n",
      "Processed prompts:  41%|▍| 102/250 [00:16<00:07, 18.68it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  42%|▍| 104/250 [00:16<00:10, 14.32it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  42%|▍| 106/250 [00:17<00:13, 10.44it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  43%|▍| 108/250 [00:17<00:13, 10.71it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  44%|▍| 110/250 [00:17<00:13, 10.65it/s, est. speed input: 22\u001b[A\n",
      "Processed prompts:  45%|▍| 112/250 [00:17<00:12, 10.94it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  46%|▍| 114/250 [00:17<00:12, 10.83it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  46%|▍| 116/250 [00:18<00:14,  9.54it/s, est. speed input: 23\u001b[A\n",
      "Processed prompts:  48%|▍| 119/250 [00:18<00:10, 12.29it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  49%|▍| 122/250 [00:18<00:08, 14.79it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  50%|▍| 124/250 [00:18<00:09, 13.16it/s, est. speed input: 25\u001b[A\n",
      "Processed prompts:  51%|▌| 128/250 [00:18<00:07, 17.00it/s, est. speed input: 25\u001b[A\n",
      "Processed prompts:  53%|▌| 132/250 [00:18<00:05, 21.60it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  54%|▌| 135/250 [00:18<00:06, 18.83it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  55%|▌| 138/250 [00:19<00:06, 18.54it/s, est. speed input: 27\u001b[A\n",
      "Processed prompts:  56%|▌| 141/250 [00:19<00:05, 18.40it/s, est. speed input: 27\u001b[A\n",
      "Processed prompts:  58%|▌| 146/250 [00:19<00:04, 24.43it/s, est. speed input: 28\u001b[A\n",
      "Processed prompts:  60%|▌| 149/250 [00:19<00:04, 24.31it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  61%|▌| 152/250 [00:19<00:04, 23.43it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  62%|▌| 155/250 [00:19<00:04, 20.78it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  63%|▋| 158/250 [00:20<00:05, 17.42it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  64%|▋| 161/250 [00:20<00:04, 19.41it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  66%|▋| 164/250 [00:20<00:06, 14.33it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  66%|▋| 166/250 [00:20<00:08, 10.11it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  67%|▋| 168/250 [00:21<00:07, 11.09it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  69%|▋| 172/250 [00:21<00:05, 13.77it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  70%|▋| 174/250 [00:21<00:07,  9.70it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  70%|▋| 176/250 [00:21<00:06, 11.07it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  72%|▋| 179/250 [00:21<00:05, 13.92it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  73%|▋| 182/250 [00:22<00:04, 16.72it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  75%|▋| 187/250 [00:22<00:02, 21.06it/s, est. speed input: 33\u001b[A\n",
      "Processed prompts:  76%|▊| 190/250 [00:22<00:05, 10.99it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  77%|▊| 192/250 [00:23<00:07,  7.36it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  78%|▊| 194/250 [00:23<00:07,  7.86it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  78%|▊| 196/250 [00:23<00:06,  7.93it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  79%|▊| 198/250 [00:24<00:06,  8.62it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  80%|▊| 200/250 [00:24<00:07,  6.75it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  80%|▊| 201/250 [00:24<00:07,  6.88it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  81%|▊| 202/250 [00:25<00:10,  4.77it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  81%|▊| 203/250 [00:25<00:10,  4.66it/s, est. speed input: 32\u001b[A\n",
      "Processed prompts:  82%|▊| 204/250 [00:26<00:15,  2.89it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  82%|▊| 206/250 [00:26<00:11,  3.86it/s, est. speed input: 31\u001b[A\n",
      "Processed prompts:  83%|▊| 208/250 [00:27<00:12,  3.44it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  84%|▊| 209/250 [00:27<00:10,  3.96it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  84%|▊| 211/250 [00:28<00:12,  3.04it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  85%|▊| 213/250 [00:28<00:10,  3.68it/s, est. speed input: 30\u001b[A\n",
      "Processed prompts:  86%|▊| 214/250 [00:29<00:15,  2.34it/s, est. speed input: 29\u001b[A\n",
      "Processed prompts:  86%|▊| 215/250 [00:30<00:19,  1.79it/s, est. speed input: 28\u001b[A\n",
      "Processed prompts:  86%|▊| 216/250 [00:31<00:22,  1.49it/s, est. speed input: 27\u001b[A\n",
      "Processed prompts:  87%|▊| 217/250 [00:33<00:32,  1.02it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  87%|▊| 218/250 [00:34<00:28,  1.14it/s, est. speed input: 26\u001b[A\n",
      "Processed prompts:  88%|▉| 219/250 [00:34<00:27,  1.13it/s, est. speed input: 25\u001b[A\n",
      "Processed prompts:  88%|▉| 220/250 [00:35<00:20,  1.45it/s, est. speed input: 25\u001b[A\n",
      "Processed prompts:  89%|▉| 222/250 [00:35<00:12,  2.24it/s, est. speed input: 25\u001b[A\n",
      "Processed prompts:  89%|▉| 223/250 [00:35<00:11,  2.28it/s, est. speed input: 25\u001b[A\n",
      "Processed prompts:  90%|▉| 224/250 [00:37<00:16,  1.58it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  90%|▉| 225/250 [00:37<00:17,  1.44it/s, est. speed input: 24\u001b[A\n",
      "Processed prompts:  90%|▉| 226/250 [00:41<00:36,  1.50s/it, est. speed input: 22\u001b[A\n",
      "Processed prompts:  91%|▉| 227/250 [00:44<00:43,  1.90s/it, est. speed input: 21\u001b[A\n",
      "Processed prompts:  91%|▉| 228/250 [00:44<00:32,  1.49s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts:  92%|▉| 229/250 [00:45<00:23,  1.12s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts:  92%|▉| 230/250 [00:46<00:24,  1.25s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts:  92%|▉| 231/250 [00:47<00:24,  1.27s/it, est. speed input: 19\u001b[A\n",
      "Processed prompts:  93%|▉| 232/250 [00:48<00:20,  1.13s/it, est. speed input: 19\u001b[A\n",
      "Processed prompts:  93%|▉| 233/250 [00:49<00:15,  1.12it/s, est. speed input: 19\u001b[A\n",
      "Processed prompts:  94%|▉| 234/250 [00:58<00:53,  3.36s/it, est. speed input: 16\u001b[A\n",
      "Processed prompts:  94%|▉| 235/250 [00:59<00:40,  2.68s/it, est. speed input: 16\u001b[A\n",
      "Processed prompts:  94%|▉| 236/250 [01:05<00:51,  3.66s/it, est. speed input: 14\u001b[A\n",
      "Processed prompts:  95%|▉| 237/250 [01:07<00:43,  3.35s/it, est. speed input: 14\u001b[A\n",
      "Processed prompts:  95%|▉| 238/250 [01:09<00:34,  2.89s/it, est. speed input: 14\u001b[A\n",
      "Processed prompts:  96%|▉| 239/250 [01:13<00:35,  3.22s/it, est. speed input: 13\u001b[A\n",
      "Processed prompts:  96%|▉| 240/250 [01:29<01:10,  7.04s/it, est. speed input: 11\u001b[A\n",
      "Processed prompts:  96%|▉| 241/250 [01:41<01:14,  8.32s/it, est. speed input: 99\u001b[A\n",
      "Processed prompts:  97%|▉| 242/250 [01:45<00:56,  7.03s/it, est. speed input: 96\u001b[A\n",
      "Processed prompts:  97%|▉| 243/250 [02:19<01:46, 15.21s/it, est. speed input: 72\u001b[A\n",
      "Processed prompts:  98%|▉| 244/250 [02:29<01:22, 13.82s/it, est. speed input: 68\u001b[A\n",
      "Processed prompts:  98%|▉| 245/250 [03:16<01:57, 23.54s/it, est. speed input: 52\u001b[A\n",
      "Processed prompts:  98%|▉| 246/250 [03:29<01:21, 20.43s/it, est. speed input: 49\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [03:29<00:00,  1.19it/s, est. speed input: 49\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [03:29<00:00,  1.19it/s]\n",
      "2026-01-08:16:07:32 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-08:16:07:32 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: None, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_te|      3|flexible-extract|     0|exact_match|↑  |0.072|±  |0.0164|\n",
      "|              |       |strict-match    |     0|exact_match|↑  |0.000|±  |0.0000|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_te\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_te \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7522c781-a6d9-4e7a-96c8-c39b69512219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08:16:19:12 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-08:16:19:52 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-08:16:21:19 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_th']\n",
      "2026-01-08:16:21:19 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-08:16:21:19 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-08:16:21:19 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:25\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:25\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:25\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:27\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:28\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:29\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:44603 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:29\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:30\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:30\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:31\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.83it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.83it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:32\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.60 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:32\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.436911 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:38\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/f195ca22f3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:38\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 5.36 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:42\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.369 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:42\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 6.73 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:42\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:43\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:43\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 37.68it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:46\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5268)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:46\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 13.73 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:21:46\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "2026-01-08:16:21:51 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-08:16:21:51 INFO     [tasks:691] Task: mgsm_en_cot_th (mgsm/en_cot/mgsm_en_cot_th.yaml)\n",
      "2026-01-08:16:21:51 INFO     [evaluator:313] mgsm_en_cot_th: Using gen_kwargs: {'do_sample': True, 'until': ['โจทย์:', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-08:16:21:51 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-08:16:21:51 INFO     [api.task:310] Building contexts for mgsm_en_cot_th on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 485.95it/s]\n",
      "2026-01-08:16:21:52 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████| 250/250 [00:00<00:00, 8535.70it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:02<10:10,  2.45s/it, est. speed input: 41.5\u001b[A\n",
      "Processed prompts:   1%| | 2/250 [00:03<06:46,  1.64s/it, est. speed input: 98.2\u001b[A\n",
      "Processed prompts:   2%| | 4/250 [00:03<02:59,  1.37it/s, est. speed input: 145.\u001b[A\n",
      "Processed prompts:   2%| | 5/250 [00:05<03:28,  1.18it/s, est. speed input: 159.\u001b[A\n",
      "Processed prompts:   2%| | 6/250 [00:05<02:33,  1.59it/s, est. speed input: 186.\u001b[A\n",
      "Processed prompts:   3%| | 7/250 [00:05<01:59,  2.03it/s, est. speed input: 206.\u001b[A\n",
      "Processed prompts:   3%| | 8/250 [00:05<01:31,  2.65it/s, est. speed input: 226.\u001b[A\n",
      "Processed prompts:   4%| | 9/250 [00:05<01:14,  3.23it/s, est. speed input: 253.\u001b[A\n",
      "Processed prompts:   4%| | 10/250 [00:05<01:06,  3.61it/s, est. speed input: 261\u001b[A\n",
      "Processed prompts:   4%| | 11/250 [00:05<00:55,  4.29it/s, est. speed input: 270\u001b[A\n",
      "Processed prompts:   5%| | 12/250 [00:06<00:48,  4.92it/s, est. speed input: 281\u001b[A\n",
      "Processed prompts:   5%| | 13/250 [00:06<00:44,  5.28it/s, est. speed input: 308\u001b[A\n",
      "Processed prompts:   6%| | 14/250 [00:06<01:09,  3.40it/s, est. speed input: 295\u001b[A\n",
      "Processed prompts:   6%| | 16/250 [00:06<00:44,  5.28it/s, est. speed input: 340\u001b[A\n",
      "Processed prompts:   7%| | 18/250 [00:07<00:32,  7.06it/s, est. speed input: 372\u001b[A\n",
      "Processed prompts:   8%| | 19/250 [00:07<00:34,  6.66it/s, est. speed input: 383\u001b[A\n",
      "Processed prompts:  10%| | 24/250 [00:07<00:17, 13.28it/s, est. speed input: 453\u001b[A\n",
      "Processed prompts:  10%| | 26/250 [00:07<00:19, 11.78it/s, est. speed input: 496\u001b[A\n",
      "Processed prompts:  11%| | 28/250 [00:07<00:19, 11.65it/s, est. speed input: 517\u001b[A\n",
      "Processed prompts:  13%|▏| 32/250 [00:07<00:13, 15.86it/s, est. speed input: 574\u001b[A\n",
      "Processed prompts:  14%|▏| 34/250 [00:08<00:14, 14.53it/s, est. speed input: 594\u001b[A\n",
      "Processed prompts:  15%|▏| 37/250 [00:08<00:14, 14.78it/s, est. speed input: 630\u001b[A\n",
      "Processed prompts:  16%|▏| 40/250 [00:08<00:15, 13.92it/s, est. speed input: 682\u001b[A\n",
      "Processed prompts:  17%|▏| 42/250 [00:08<00:15, 13.19it/s, est. speed input: 709\u001b[A\n",
      "Processed prompts:  18%|▏| 44/250 [00:08<00:16, 12.74it/s, est. speed input: 730\u001b[A\n",
      "Processed prompts:  18%|▏| 46/250 [00:08<00:15, 13.33it/s, est. speed input: 754\u001b[A\n",
      "Processed prompts:  19%|▏| 48/250 [00:09<00:19, 10.13it/s, est. speed input: 753\u001b[A\n",
      "Processed prompts:  20%|▏| 50/250 [00:09<00:20,  9.89it/s, est. speed input: 761\u001b[A\n",
      "Processed prompts:  22%|▏| 54/250 [00:09<00:15, 12.49it/s, est. speed input: 815\u001b[A\n",
      "Processed prompts:  22%|▏| 56/250 [00:09<00:15, 12.27it/s, est. speed input: 833\u001b[A\n",
      "Processed prompts:  23%|▏| 58/250 [00:10<00:15, 12.12it/s, est. speed input: 839\u001b[A\n",
      "Processed prompts:  24%|▏| 60/250 [00:10<00:15, 12.02it/s, est. speed input: 858\u001b[A\n",
      "Processed prompts:  25%|▏| 62/250 [00:10<00:16, 11.51it/s, est. speed input: 866\u001b[A\n",
      "Processed prompts:  26%|▎| 65/250 [00:10<00:12, 14.79it/s, est. speed input: 903\u001b[A\n",
      "Processed prompts:  27%|▎| 67/250 [00:10<00:13, 13.84it/s, est. speed input: 910\u001b[A\n",
      "Processed prompts:  28%|▎| 69/250 [00:10<00:13, 13.70it/s, est. speed input: 926\u001b[A\n",
      "Processed prompts:  28%|▎| 71/250 [00:11<00:13, 13.59it/s, est. speed input: 933\u001b[A\n",
      "Processed prompts:  29%|▎| 73/250 [00:11<00:18,  9.36it/s, est. speed input: 934\u001b[A\n",
      "Processed prompts:  30%|▎| 76/250 [00:11<00:13, 12.57it/s, est. speed input: 972\u001b[A\n",
      "Processed prompts:  32%|▎| 80/250 [00:11<00:09, 17.55it/s, est. speed input: 100\u001b[A\n",
      "Processed prompts:  33%|▎| 83/250 [00:11<00:08, 19.25it/s, est. speed input: 104\u001b[A\n",
      "Processed prompts:  34%|▎| 86/250 [00:11<00:09, 17.24it/s, est. speed input: 106\u001b[A\n",
      "Processed prompts:  36%|▎| 90/250 [00:12<00:07, 21.08it/s, est. speed input: 110\u001b[A\n",
      "Processed prompts:  37%|▎| 93/250 [00:12<00:08, 18.95it/s, est. speed input: 112\u001b[A\n",
      "Processed prompts:  38%|▍| 96/250 [00:12<00:08, 18.34it/s, est. speed input: 114\u001b[A\n",
      "Processed prompts:  40%|▍| 99/250 [00:12<00:08, 17.94it/s, est. speed input: 116\u001b[A\n",
      "Processed prompts:  41%|▍| 103/250 [00:12<00:06, 21.75it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  42%|▍| 106/250 [00:12<00:07, 19.68it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  44%|▍| 109/250 [00:13<00:07, 19.43it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  45%|▍| 113/250 [00:13<00:06, 20.79it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  46%|▍| 116/250 [00:13<00:08, 15.27it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  48%|▍| 120/250 [00:13<00:07, 16.86it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  49%|▍| 122/250 [00:14<00:09, 13.63it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  50%|▍| 124/250 [00:14<00:11, 11.44it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  50%|▌| 126/250 [00:14<00:12,  9.55it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  51%|▌| 128/250 [00:15<00:17,  6.83it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  52%|▌| 131/250 [00:15<00:13,  8.82it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  53%|▌| 133/250 [00:15<00:14,  8.20it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  55%|▌| 137/250 [00:15<00:10, 10.63it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  56%|▌| 140/250 [00:16<00:09, 12.13it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  57%|▌| 142/250 [00:16<00:08, 13.26it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  58%|▌| 144/250 [00:16<00:12,  8.24it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  58%|▌| 146/250 [00:16<00:13,  7.90it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  59%|▌| 148/250 [00:17<00:15,  6.62it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  60%|▌| 149/250 [00:17<00:15,  6.40it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  60%|▌| 151/250 [00:17<00:14,  6.72it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  62%|▌| 154/250 [00:18<00:12,  7.89it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  63%|▋| 157/250 [00:18<00:08, 10.57it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  64%|▋| 159/250 [00:18<00:12,  7.42it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  64%|▋| 161/250 [00:18<00:10,  8.28it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  66%|▋| 165/250 [00:19<00:07, 10.73it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  67%|▋| 167/250 [00:19<00:07, 10.45it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  68%|▋| 169/250 [00:19<00:08,  9.52it/s, est. speed input: 13\u001b[A\n",
      "Processed prompts:  68%|▋| 171/250 [00:20<00:12,  6.09it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  69%|▋| 172/250 [00:20<00:12,  6.45it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  69%|▋| 173/250 [00:20<00:19,  4.03it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  70%|▋| 174/250 [00:21<00:26,  2.86it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  70%|▋| 176/250 [00:21<00:18,  3.90it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  71%|▋| 177/250 [00:22<00:20,  3.61it/s, est. speed input: 12\u001b[A\n",
      "Processed prompts:  71%|▋| 178/250 [00:22<00:22,  3.26it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  72%|▋| 179/250 [00:22<00:18,  3.83it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  72%|▋| 180/250 [00:23<00:17,  3.95it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  72%|▋| 181/250 [00:23<00:15,  4.43it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  73%|▋| 182/250 [00:23<00:14,  4.58it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  73%|▋| 183/250 [00:24<00:25,  2.65it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  74%|▋| 184/250 [00:24<00:19,  3.35it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  74%|▋| 185/250 [00:24<00:24,  2.61it/s, est. speed input: 11\u001b[A\n",
      "Processed prompts:  74%|▋| 186/250 [00:26<00:52,  1.23it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  75%|▊| 188/250 [00:27<00:32,  1.89it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  76%|▊| 189/250 [00:28<00:39,  1.56it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  76%|▊| 190/250 [00:29<00:44,  1.34it/s, est. speed input: 10\u001b[A\n",
      "Processed prompts:  76%|▊| 191/250 [00:30<00:55,  1.06it/s, est. speed input: 96\u001b[A\n",
      "Processed prompts:  77%|▊| 192/250 [00:31<00:48,  1.20it/s, est. speed input: 95\u001b[A\n",
      "Processed prompts:  77%|▊| 193/250 [00:31<00:47,  1.21it/s, est. speed input: 93\u001b[A\n",
      "Processed prompts:  78%|▊| 194/250 [00:32<00:48,  1.16it/s, est. speed input: 91\u001b[A\n",
      "Processed prompts:  78%|▊| 195/250 [00:37<01:54,  2.09s/it, est. speed input: 80\u001b[A\n",
      "Processed prompts:  78%|▊| 196/250 [00:38<01:32,  1.72s/it, est. speed input: 78\u001b[A\n",
      "Processed prompts:  79%|▊| 197/250 [00:39<01:18,  1.49s/it, est. speed input: 77\u001b[A\n",
      "Processed prompts:  79%|▊| 198/250 [00:40<01:12,  1.39s/it, est. speed input: 75\u001b[A\n",
      "Processed prompts:  80%|▊| 199/250 [00:41<00:59,  1.18s/it, est. speed input: 74\u001b[A\n",
      "Processed prompts:  80%|▊| 200/250 [00:43<01:09,  1.39s/it, est. speed input: 71\u001b[A\n",
      "Processed prompts:  80%|▊| 201/250 [00:43<00:53,  1.10s/it, est. speed input: 71\u001b[A\n",
      "Processed prompts:  81%|▊| 202/250 [00:45<00:54,  1.13s/it, est. speed input: 70\u001b[A\n",
      "Processed prompts:  81%|▊| 203/250 [00:52<02:26,  3.12s/it, est. speed input: 60\u001b[A\n",
      "Processed prompts:  82%|▊| 204/250 [00:56<02:27,  3.20s/it, est. speed input: 56\u001b[A\n",
      "Processed prompts:  82%|▊| 205/250 [01:02<03:04,  4.10s/it, est. speed input: 51\u001b[A\n",
      "Processed prompts:  82%|▊| 206/250 [01:09<03:36,  4.92s/it, est. speed input: 46\u001b[A\n",
      "Processed prompts:  83%|▊| 207/250 [01:48<10:57, 15.29s/it, est. speed input: 29\u001b[A\n",
      "Processed prompts:  83%|▊| 208/250 [01:59<09:45, 13.94s/it, est. speed input: 27\u001b[A\n",
      "Processed prompts:  84%|▊| 209/250 [02:12<09:18, 13.63s/it, est. speed input: 24\u001b[A\n",
      "Processed prompts:  84%|▊| 210/250 [02:23<08:39, 12.98s/it, est. speed input: 22\u001b[A\n",
      "Processed prompts:  84%|▊| 211/250 [02:38<08:42, 13.41s/it, est. speed input: 20\u001b[A\n",
      "Processed prompts:  85%|▊| 212/250 [02:54<08:58, 14.17s/it, est. speed input: 19\u001b[A\n",
      "Processed prompts:  85%|▊| 213/250 [02:56<06:33, 10.62s/it, est. speed input: 18\u001b[A\n",
      "Processed prompts:  86%|▊| 214/250 [03:06<06:09, 10.27s/it, est. speed input: 18\u001b[A\n",
      "Processed prompts:  86%|▊| 215/250 [03:23<07:17, 12.50s/it, est. speed input: 16\u001b[A\n",
      "Processed prompts:  86%|▊| 216/250 [03:36<07:02, 12.44s/it, est. speed input: 15\u001b[A\n",
      "Processed prompts:  87%|▊| 217/250 [03:43<06:03, 11.02s/it, est. speed input: 15\u001b[A\n",
      "Processed prompts:  87%|▊| 218/250 [03:44<04:11,  7.87s/it, est. speed input: 15\u001b[A\n",
      "Processed prompts:  88%|▉| 219/250 [03:56<04:43,  9.14s/it, est. speed input: 14\u001b[A\n",
      "Processed prompts:  88%|▉| 220/250 [04:01<04:01,  8.05s/it, est. speed input: 14\u001b[A\n",
      "Processed prompts:  88%|▉| 221/250 [04:11<04:07,  8.55s/it, est. speed input: 13\u001b[A\n",
      "Processed prompts:  89%|▉| 222/250 [04:43<07:12, 15.44s/it, est. speed input: 12\u001b[A\n",
      "Processed prompts:  89%|▉| 223/250 [04:56<06:39, 14.80s/it, est. speed input: 11\u001b[A\n",
      "Processed prompts:  90%|▉| 224/250 [05:04<05:35, 12.92s/it, est. speed input: 11\u001b[A\n",
      "Processed prompts:  90%|▉| 225/250 [05:11<04:35, 11.01s/it, est. speed input: 11\u001b[A\n",
      "Processed prompts:  90%|▉| 226/250 [05:32<05:37, 14.05s/it, est. speed input: 10\u001b[A\n",
      "Processed prompts:  91%|▉| 227/250 [05:54<06:14, 16.27s/it, est. speed input: 10\u001b[A\n",
      "Processed prompts:  91%|▉| 228/250 [06:21<07:13, 19.70s/it, est. speed input: 93\u001b[A\n",
      "Processed prompts:  98%|▉| 246/250 [06:21<00:09,  2.27s/it, est. speed input: 10\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [06:21<00:00,  1.53s/it, est. speed input: 10\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [06:21<00:00,  1.53s/it]\n",
      "2026-01-08:16:28:16 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-08:16:28:16 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: None, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_th|      3|flexible-extract|     0|exact_match|↑  | 0.16|±  |0.0232|\n",
      "|              |       |strict-match    |     0|exact_match|↑  | 0.00|±  |0.0000|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_th\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_th \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e257bbf5-1016-44ca-8f31-75b64eabd23e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08:16:29:37 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-08:16:29:56 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-08:16:30:34 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_zh']\n",
      "2026-01-08:16:30:34 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-08:16:30:34 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-08:16:30:34 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:39\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:40\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:40\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:40\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:41\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:42\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.20.0.2:51871 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:42\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:43\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:43\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:44\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.85it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.85it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:45\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.60 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:45\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.401814 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:51\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/f195ca22f3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:51\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 5.32 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:54\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.110 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:54\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 6.43 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:55\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:55\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:55\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 35.34it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:58\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=5627)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:58\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 13.07 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-08 16:30:58\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "zh/train-00000-of-00001.parquet: 100%|█████| 5.61k/5.61k [00:00<00:00, 7.61kB/s]\n",
      "zh/test-00000-of-00001.parquet: 100%|███████| 40.8k/40.8k [00:00<00:00, 122kB/s]\n",
      "Generating train split: 100%|████████████| 8/8 [00:00<00:00, 1560.60 examples/s]\n",
      "Generating test split: 100%|███████| 250/250 [00:00<00:00, 101321.48 examples/s]\n",
      "2026-01-08:16:31:05 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-08:16:31:05 INFO     [tasks:691] Task: mgsm_en_cot_zh (mgsm/en_cot/mgsm_en_cot_zh.yaml)\n",
      "2026-01-08:16:31:05 INFO     [evaluator:313] mgsm_en_cot_zh: Using gen_kwargs: {'do_sample': True, 'until': ['问题：', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-08:16:31:05 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-08:16:31:05 INFO     [api.task:310] Building contexts for mgsm_en_cot_zh on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 487.05it/s]\n",
      "2026-01-08:16:31:05 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████| 250/250 [00:00<00:00, 8552.82it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:00<03:32,  1.17it/s, est. speed input: 77.2\u001b[A\n",
      "Processed prompts:   2%| | 5/250 [00:01<00:41,  5.97it/s, est. speed input: 329.\u001b[A\n",
      "Processed prompts:   3%| | 7/250 [00:01<00:33,  7.29it/s, est. speed input: 440.\u001b[A\n",
      "Processed prompts:   4%| | 9/250 [00:02<01:03,  3.80it/s, est. speed input: 304.\u001b[A\n",
      "Processed prompts:   5%| | 12/250 [00:02<00:43,  5.52it/s, est. speed input: 359\u001b[A\n",
      "Processed prompts:   6%| | 14/250 [00:02<00:39,  6.04it/s, est. speed input: 382\u001b[A\n",
      "Processed prompts:   6%| | 15/250 [00:02<00:39,  5.96it/s, est. speed input: 400\u001b[A\n",
      "Processed prompts:   6%| | 16/250 [00:03<00:47,  4.97it/s, est. speed input: 377\u001b[A\n",
      "Processed prompts:   8%| | 19/250 [00:03<00:31,  7.32it/s, est. speed input: 436\u001b[A\n",
      "Processed prompts:   8%| | 20/250 [00:03<00:40,  5.74it/s, est. speed input: 434\u001b[A\n",
      "Processed prompts:   8%| | 21/250 [00:04<00:56,  4.05it/s, est. speed input: 393\u001b[A\n",
      "Processed prompts:   9%| | 22/250 [00:04<00:55,  4.11it/s, est. speed input: 399\u001b[A\n",
      "Processed prompts:   9%| | 23/250 [00:04<01:04,  3.54it/s, est. speed input: 378\u001b[A\n",
      "Processed prompts:  10%| | 25/250 [00:05<00:43,  5.13it/s, est. speed input: 407\u001b[A\n",
      "Processed prompts:  10%| | 26/250 [00:05<00:38,  5.76it/s, est. speed input: 417\u001b[A\n",
      "Processed prompts:  11%| | 27/250 [00:05<00:34,  6.42it/s, est. speed input: 425\u001b[A\n",
      "Processed prompts:  12%| | 29/250 [00:05<00:31,  7.04it/s, est. speed input: 430\u001b[A\n",
      "Processed prompts:  12%| | 31/250 [00:05<00:33,  6.57it/s, est. speed input: 438\u001b[A\n",
      "Processed prompts:  13%|▏| 33/250 [00:05<00:27,  7.81it/s, est. speed input: 450\u001b[A\n",
      "Processed prompts:  14%|▏| 36/250 [00:06<00:20, 10.36it/s, est. speed input: 471\u001b[A\n",
      "Processed prompts:  15%|▏| 38/250 [00:06<00:17, 12.00it/s, est. speed input: 492\u001b[A\n",
      "Processed prompts:  16%|▏| 40/250 [00:06<00:18, 11.64it/s, est. speed input: 496\u001b[A\n",
      "Processed prompts:  17%|▏| 43/250 [00:06<00:13, 15.16it/s, est. speed input: 522\u001b[A\n",
      "Processed prompts:  18%|▏| 45/250 [00:06<00:14, 14.29it/s, est. speed input: 538\u001b[A\n",
      "Processed prompts:  20%|▏| 49/250 [00:06<00:11, 17.56it/s, est. speed input: 581\u001b[A\n",
      "Processed prompts:  20%|▏| 51/250 [00:07<00:13, 14.42it/s, est. speed input: 592\u001b[A\n",
      "Processed prompts:  21%|▏| 53/250 [00:07<00:19, 10.04it/s, est. speed input: 583\u001b[A\n",
      "Processed prompts:  23%|▏| 57/250 [00:07<00:14, 13.42it/s, est. speed input: 624\u001b[A\n",
      "Processed prompts:  24%|▏| 59/250 [00:07<00:13, 14.02it/s, est. speed input: 633\u001b[A\n",
      "Processed prompts:  26%|▎| 65/250 [00:07<00:09, 20.48it/s, est. speed input: 688\u001b[A\n",
      "Processed prompts:  27%|▎| 68/250 [00:08<00:19,  9.57it/s, est. speed input: 660\u001b[A\n",
      "Processed prompts:  29%|▎| 72/250 [00:08<00:14, 12.56it/s, est. speed input: 698\u001b[A\n",
      "Processed prompts:  30%|▎| 75/250 [00:09<00:13, 12.54it/s, est. speed input: 708\u001b[A\n",
      "Processed prompts:  31%|▎| 77/250 [00:09<00:13, 13.23it/s, est. speed input: 725\u001b[A\n",
      "Processed prompts:  32%|▎| 79/250 [00:09<00:13, 12.69it/s, est. speed input: 728\u001b[A\n",
      "Processed prompts:  33%|▎| 82/250 [00:09<00:12, 13.81it/s, est. speed input: 750\u001b[A\n",
      "Processed prompts:  34%|▎| 85/250 [00:09<00:10, 16.20it/s, est. speed input: 787\u001b[A\n",
      "Processed prompts:  35%|▎| 87/250 [00:09<00:11, 13.87it/s, est. speed input: 793\u001b[A\n",
      "Processed prompts:  36%|▎| 89/250 [00:10<00:12, 12.65it/s, est. speed input: 800\u001b[A\n",
      "Processed prompts:  36%|▎| 91/250 [00:10<00:21,  7.52it/s, est. speed input: 774\u001b[A\n",
      "Processed prompts:  37%|▎| 93/250 [00:10<00:18,  8.51it/s, est. speed input: 785\u001b[A\n",
      "Processed prompts:  38%|▍| 95/250 [00:11<00:28,  5.36it/s, est. speed input: 763\u001b[A\n",
      "Processed prompts:  39%|▍| 97/250 [00:11<00:25,  6.09it/s, est. speed input: 765\u001b[A\n",
      "Processed prompts:  39%|▍| 98/250 [00:11<00:23,  6.41it/s, est. speed input: 766\u001b[A\n",
      "Processed prompts:  40%|▍| 101/250 [00:11<00:16,  9.14it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  41%|▍| 103/250 [00:12<00:20,  7.34it/s, est. speed input: 76\u001b[A\n",
      "Processed prompts:  42%|▍| 106/250 [00:12<00:14,  9.63it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  43%|▍| 108/250 [00:12<00:14, 10.10it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  44%|▍| 110/250 [00:12<00:12, 11.11it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  45%|▍| 112/250 [00:13<00:13, 10.36it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  46%|▍| 114/250 [00:13<00:16,  8.29it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  46%|▍| 116/250 [00:13<00:13,  9.81it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  47%|▍| 118/250 [00:13<00:15,  8.45it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  48%|▍| 120/250 [00:14<00:16,  8.00it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  48%|▍| 121/250 [00:14<00:15,  8.14it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  49%|▍| 123/250 [00:14<00:13,  9.62it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  50%|▌| 126/250 [00:14<00:10, 12.00it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  51%|▌| 128/250 [00:14<00:10, 11.41it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  52%|▌| 130/250 [00:14<00:11, 10.45it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  53%|▌| 132/250 [00:15<00:10, 11.06it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  54%|▌| 134/250 [00:15<00:09, 11.94it/s, est. speed input: 79\u001b[A\n",
      "Processed prompts:  54%|▌| 136/250 [00:15<00:10, 10.91it/s, est. speed input: 79\u001b[A\n",
      "Processed prompts:  55%|▌| 138/250 [00:15<00:14,  7.51it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  56%|▌| 140/250 [00:16<00:14,  7.77it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  57%|▌| 143/250 [00:16<00:10,  9.74it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  58%|▌| 146/250 [00:16<00:08, 12.83it/s, est. speed input: 79\u001b[A\n",
      "Processed prompts:  59%|▌| 148/250 [00:16<00:09, 10.34it/s, est. speed input: 79\u001b[A\n",
      "Processed prompts:  60%|▌| 150/250 [00:17<00:10,  9.57it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  61%|▌| 153/250 [00:17<00:08, 11.84it/s, est. speed input: 79\u001b[A\n",
      "Processed prompts:  62%|▌| 155/250 [00:17<00:11,  8.10it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  64%|▋| 159/250 [00:17<00:07, 11.74it/s, est. speed input: 79\u001b[A\n",
      "Processed prompts:  64%|▋| 161/250 [00:17<00:07, 11.84it/s, est. speed input: 80\u001b[A\n",
      "Processed prompts:  65%|▋| 163/250 [00:18<00:11,  7.49it/s, est. speed input: 78\u001b[A\n",
      "Processed prompts:  66%|▋| 165/250 [00:18<00:13,  6.35it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  66%|▋| 166/250 [00:19<00:14,  5.92it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  67%|▋| 167/250 [00:19<00:16,  5.02it/s, est. speed input: 76\u001b[A\n",
      "Processed prompts:  68%|▋| 169/250 [00:19<00:12,  6.58it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  69%|▋| 172/250 [00:19<00:09,  7.90it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  69%|▋| 173/250 [00:20<00:09,  7.79it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  70%|▋| 174/250 [00:20<00:09,  7.85it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  70%|▋| 175/250 [00:20<00:09,  7.87it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  71%|▋| 177/250 [00:20<00:08,  8.76it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  71%|▋| 178/250 [00:20<00:08,  8.46it/s, est. speed input: 76\u001b[A\n",
      "Processed prompts:  72%|▋| 179/250 [00:20<00:09,  7.22it/s, est. speed input: 76\u001b[A\n",
      "Processed prompts:  72%|▋| 180/250 [00:20<00:09,  7.49it/s, est. speed input: 76\u001b[A\n",
      "Processed prompts:  73%|▋| 182/250 [00:21<00:07,  8.75it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  73%|▋| 183/250 [00:21<00:08,  7.82it/s, est. speed input: 77\u001b[A\n",
      "Processed prompts:  74%|▋| 184/250 [00:21<00:08,  7.72it/s, est. speed input: 76\u001b[A\n",
      "Processed prompts:  74%|▋| 186/250 [00:21<00:11,  5.68it/s, est. speed input: 75\u001b[A\n",
      "Processed prompts:  75%|▋| 187/250 [00:22<00:10,  5.93it/s, est. speed input: 75\u001b[A\n",
      "Processed prompts:  75%|▊| 188/250 [00:22<00:11,  5.56it/s, est. speed input: 75\u001b[A\n",
      "Processed prompts:  76%|▊| 189/250 [00:22<00:12,  4.89it/s, est. speed input: 74\u001b[A\n",
      "Processed prompts:  77%|▊| 192/250 [00:22<00:09,  6.38it/s, est. speed input: 74\u001b[A\n",
      "Processed prompts:  77%|▊| 193/250 [00:22<00:08,  6.60it/s, est. speed input: 74\u001b[A\n",
      "Processed prompts:  78%|▊| 196/250 [00:23<00:09,  5.89it/s, est. speed input: 73\u001b[A\n",
      "Processed prompts:  79%|▊| 197/250 [00:23<00:08,  5.93it/s, est. speed input: 73\u001b[A\n",
      "Processed prompts:  79%|▊| 198/250 [00:24<00:12,  4.01it/s, est. speed input: 72\u001b[A\n",
      "Processed prompts:  80%|▊| 200/250 [00:24<00:12,  3.93it/s, est. speed input: 71\u001b[A\n",
      "Processed prompts:  80%|▊| 201/250 [00:25<00:12,  3.84it/s, est. speed input: 71\u001b[A\n",
      "Processed prompts:  81%|▊| 203/250 [00:25<00:11,  4.15it/s, est. speed input: 70\u001b[A\n",
      "Processed prompts:  82%|▊| 204/250 [00:25<00:10,  4.47it/s, est. speed input: 70\u001b[A\n",
      "Processed prompts:  82%|▊| 206/250 [00:26<00:09,  4.69it/s, est. speed input: 70\u001b[A\n",
      "Processed prompts:  83%|▊| 207/250 [00:26<00:09,  4.54it/s, est. speed input: 70\u001b[A\n",
      "Processed prompts:  83%|▊| 208/250 [00:26<00:10,  3.95it/s, est. speed input: 69\u001b[A\n",
      "Processed prompts:  84%|▊| 209/250 [00:26<00:09,  4.38it/s, est. speed input: 69\u001b[A\n",
      "Processed prompts:  84%|▊| 210/250 [00:27<00:08,  4.63it/s, est. speed input: 69\u001b[A\n",
      "Processed prompts:  84%|▊| 211/250 [00:27<00:07,  5.42it/s, est. speed input: 69\u001b[A\n",
      "Processed prompts:  85%|▊| 212/250 [00:27<00:08,  4.55it/s, est. speed input: 68\u001b[A\n",
      "Processed prompts:  85%|▊| 213/250 [00:27<00:09,  3.85it/s, est. speed input: 68\u001b[A\n",
      "Processed prompts:  86%|▊| 214/250 [00:28<00:10,  3.52it/s, est. speed input: 67\u001b[A\n",
      "Processed prompts:  86%|▊| 215/250 [00:28<00:08,  4.30it/s, est. speed input: 68\u001b[A\n",
      "Processed prompts:  87%|▊| 217/250 [00:28<00:05,  6.05it/s, est. speed input: 68\u001b[A\n",
      "Processed prompts:  87%|▊| 218/250 [00:28<00:07,  4.14it/s, est. speed input: 67\u001b[A\n",
      "Processed prompts:  88%|▉| 219/250 [00:29<00:11,  2.76it/s, est. speed input: 66\u001b[A\n",
      "Processed prompts:  88%|▉| 220/250 [00:30<00:15,  1.99it/s, est. speed input: 65\u001b[A\n",
      "Processed prompts:  88%|▉| 221/250 [00:31<00:18,  1.55it/s, est. speed input: 63\u001b[A\n",
      "Processed prompts:  89%|▉| 222/250 [00:31<00:13,  2.01it/s, est. speed input: 63\u001b[A\n",
      "Processed prompts:  89%|▉| 223/250 [00:31<00:10,  2.50it/s, est. speed input: 63\u001b[A\n",
      "Processed prompts:  90%|▉| 224/250 [00:32<00:11,  2.18it/s, est. speed input: 62\u001b[A\n",
      "Processed prompts:  90%|▉| 225/250 [00:33<00:18,  1.32it/s, est. speed input: 59\u001b[A\n",
      "Processed prompts:  90%|▉| 226/250 [00:35<00:25,  1.08s/it, est. speed input: 57\u001b[A\n",
      "Processed prompts:  91%|▉| 227/250 [00:36<00:21,  1.05it/s, est. speed input: 56\u001b[A\n",
      "Processed prompts:  91%|▉| 228/250 [00:36<00:17,  1.28it/s, est. speed input: 56\u001b[A\n",
      "Processed prompts:  92%|▉| 229/250 [00:37<00:15,  1.34it/s, est. speed input: 55\u001b[A\n",
      "Processed prompts:  92%|▉| 230/250 [00:38<00:16,  1.24it/s, est. speed input: 54\u001b[A\n",
      "Processed prompts:  92%|▉| 231/250 [00:42<00:34,  1.83s/it, est. speed input: 49\u001b[A\n",
      "Processed prompts:  93%|▉| 232/250 [00:43<00:27,  1.54s/it, est. speed input: 48\u001b[A\n",
      "Processed prompts:  93%|▉| 233/250 [00:47<00:38,  2.25s/it, est. speed input: 44\u001b[A\n",
      "Processed prompts:  94%|▉| 234/250 [00:47<00:28,  1.76s/it, est. speed input: 44\u001b[A\n",
      "Processed prompts:  94%|▉| 235/250 [00:48<00:20,  1.37s/it, est. speed input: 44\u001b[A\n",
      "Processed prompts:  94%|▉| 236/250 [00:48<00:14,  1.05s/it, est. speed input: 44\u001b[A\n",
      "Processed prompts:  95%|▉| 237/250 [00:52<00:22,  1.76s/it, est. speed input: 41\u001b[A\n",
      "Processed prompts:  95%|▉| 238/250 [00:54<00:23,  1.94s/it, est. speed input: 39\u001b[A\n",
      "Processed prompts:  96%|▉| 239/250 [00:57<00:25,  2.36s/it, est. speed input: 37\u001b[A\n",
      "Processed prompts:  96%|▉| 240/250 [01:07<00:45,  4.55s/it, est. speed input: 32\u001b[A\n",
      "Processed prompts:  97%|▉| 242/250 [01:10<00:25,  3.24s/it, est. speed input: 31\u001b[A\n",
      "Processed prompts:  97%|▉| 243/250 [01:14<00:22,  3.24s/it, est. speed input: 29\u001b[A\n",
      "Processed prompts:  98%|▉| 244/250 [01:21<00:26,  4.44s/it, est. speed input: 27\u001b[A\n",
      "Processed prompts:  98%|▉| 245/250 [03:15<02:50, 34.09s/it, est. speed input: 11\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [03:15<00:00,  1.28it/s, est. speed input: 11\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [03:15<00:00,  1.28it/s]\n",
      "2026-01-08:16:34:27 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-08:16:34:27 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: None, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_zh|      3|flexible-extract|     0|exact_match|↑  |0.644|±  |0.0303|\n",
      "|              |       |strict-match    |     0|exact_match|↑  |0.000|±  |0.0000|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_zh\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_zh \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8a072-c0a7-4fd4-9bbf-19a54eb0d84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
