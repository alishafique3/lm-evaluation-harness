{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579bb623-9157-4edf-8140-fa97cabc3847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lm-eval[math] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "784535ae-5d3f-49b7-a4f1-81043aeff347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/lm-evaluation-harness/zmgsm_urdu_util'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e93c2295-8dcb-4039-bbc9-65f08dadbf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16:16:03:21 WARNING  [config.evaluate_config:281] --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2026-01-16:16:03:21 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-16:16:03:52 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-16:16:04:54 INFO     [_cli.run:376] Selected Tasks: ['commonsense_qa_urdu']\n",
      "2026-01-16:16:04:54 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-16:16:04:54 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-16:16:04:54 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 32768, 'trust_remote_code': True}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 16:04:59\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'trust_remote_code': True, 'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 16:05:10\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 16:05:10\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 16:05:10\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:05:12\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:05:12\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.158.2:41379 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:05:12\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:05:13\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:05:13\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c9379a36-df86-4a32-8899-e85f447f4c10)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/model.safetensors.index.json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m [2026-01-16 16:05:24] WARNING _http.py:319: '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c9379a36-df86-4a32-8899-e85f447f4c10)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/model.safetensors.index.json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m Retrying in 1s [Retry 1/5].\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m [2026-01-16 16:05:24] WARNING _http.py:328: Retrying in 1s [Retry 1/5].\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d5602ac8-0d3e-45d0-95a5-516cf1c0b7c7)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/model.safetensors.index.json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m [2026-01-16 16:05:35] WARNING _http.py:319: '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d5602ac8-0d3e-45d0-95a5-516cf1c0b7c7)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/model.safetensors.index.json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m Retrying in 2s [Retry 2/5].\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m [2026-01-16 16:05:35] WARNING _http.py:328: Retrying in 2s [Retry 2/5].\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9a5adbbc-518f-4ffd-82a1-2ed465850dd4)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/model.safetensors.index.json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m [2026-01-16 16:05:47] WARNING _http.py:319: '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9a5adbbc-518f-4ffd-82a1-2ed465850dd4)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/model.safetensors.index.json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m Retrying in 4s [Retry 3/5].\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m [2026-01-16 16:05:47] WARNING _http.py:328: Retrying in 4s [Retry 3/5].\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: cd56d3cf-72dc-45e8-8f44-46a1c50be9e6)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/model.safetensors.index.json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m [2026-01-16 16:06:01] WARNING _http.py:319: '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: cd56d3cf-72dc-45e8-8f44-46a1c50be9e6)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/model.safetensors.index.json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m Retrying in 8s [Retry 4/5].\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m [2026-01-16 16:06:01] WARNING _http.py:328: Retrying in 8s [Retry 4/5].\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:10\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.94it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.94it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:10\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.57 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:11\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 56.841232 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:15\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/637a19798a/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:15\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 4.58 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:19\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.108 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:19\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 5.69 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:19\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:19\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:19\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 40.72it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:22\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4303)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:22\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 11.63 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 16:06:23\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b55993cf-4aa1-4f97-8dc6-a5fca5c2f753)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/video_preprocessor_config.json\n",
      "[2026-01-16 16:06:36] WARNING _http.py:319: '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b55993cf-4aa1-4f97-8dc6-a5fca5c2f753)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/video_preprocessor_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[2026-01-16 16:06:36] WARNING _http.py:328: Retrying in 1s [Retry 1/5].\n",
      "README.md: 3.52kB [00:00, 9.33MB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "[2026-01-16 16:06:56] WARNING repocard.py:106: Repo card metadata block was not found. Setting CardData to empty.\n",
      "data/train-00000-of-00001.parquet: 100%|███| 2.40M/2.40M [00:00<00:00, 4.31MB/s]\n",
      "data/validation-00000-of-00001.parquet: 100%|█| 297k/297k [00:00<00:00, 580kB/s]\n",
      "data/test-00000-of-00001.parquet: 100%|██████| 282k/282k [00:00<00:00, 1.01MB/s]\n",
      "Generating train split: 100%|████| 9741/9741 [00:00<00:00, 302920.57 examples/s]\n",
      "Generating validation split: 100%|█| 1221/1221 [00:00<00:00, 113810.51 examples/\n",
      "Generating test split: 100%|█████| 1140/1140 [00:00<00:00, 109685.19 examples/s]\n",
      "2026-01-16:16:07:04 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-16:16:07:04 INFO     [tasks:691] Task: commonsense_qa_urdu (commonsense_qa_urdu/direct/commonsense_qa_urdu.yaml)\n",
      "2026-01-16:16:07:04 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-16:16:07:04 INFO     [api.task:310] Building contexts for commonsense_qa_urdu on rank 0...\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 320.14it/s]\n",
      "2026-01-16:16:07:04 INFO     [evaluator:583] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                    | 0/50 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|████████████████████████| 50/50 [00:00<00:00, 9595.31it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/50 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, \u001b[A\n",
      "Processed prompts:   2%| | 1/50 [00:03<03:06,  3.80s/it, est. speed input: 34.98\u001b[A\n",
      "Processed prompts:   8%| | 4/50 [00:04<00:36,  1.25it/s, est. speed input: 130.0\u001b[A\n",
      "Processed prompts: 100%|█| 50/50 [00:04<00:00, 12.22it/s, est. speed input: 1329\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████| 50/50 [00:04<00:00, 12.19it/s]\n",
      "2026-01-16:16:07:09 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-16:16:07:09 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 32768}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: 10.0, num_fewshot: None, batch_size: auto\n",
      "|       Tasks       |Version|Filter|n-shot| Metric |   |Value|   |Stderr|\n",
      "|-------------------|-------|------|-----:|--------|---|----:|---|-----:|\n",
      "|commonsense_qa_urdu|Yaml   |none  |     0|acc     |↑  |  0.1|±  |   0.1|\n",
      "|                   |       |none  |     0|acc_norm|↑  |  0.1|±  |   0.1|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm-eval --model vllm \\\n",
    "  --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=32768,trust_remote_code=True \\\n",
    "  --tasks commonsense_qa_urdu \\\n",
    "  --apply_chat_template \\\n",
    "  --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "  --batch_size 'auto' \\\n",
    "  --output_path './results/' \\\n",
    "  --log_samples \\\n",
    "  --limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fbf39b0-b3a5-4ccf-816e-bc0b401cb86d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16:15:59:36 WARNING  [config.evaluate_config:281] --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2026-01-16:15:59:36 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-16:16:00:22 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-16:16:01:53 INFO     [_cli.run:376] Selected Tasks: ['openbookqa_urdu']\n",
      "2026-01-16:16:01:53 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-16:16:01:53 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-16:16:01:53 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 32768, 'trust_remote_code': True}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 16:01:59\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'trust_remote_code': True, 'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 16:01:59\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 16:01:59\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 16:01:59\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:00\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:00\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.158.2:51115 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:00\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:01\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:01\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:02\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.99it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.99it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:02\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.56 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:03\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.003049 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:07\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/637a19798a/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:07\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 4.37 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:11\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.172 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:11\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 5.55 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:11\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:11\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:11\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 40.91it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:14\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3784)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:14\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 11.57 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 16:02:15\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "README.md: 2.50kB [00:00, 10.8MB/s]\n",
      "data/train-00000-of-00001.parquet: 100%|███| 1.10M/1.10M [00:00<00:00, 2.01MB/s]\n",
      "data/validation-00000-of-00001.parquet: 100%|█| 127k/127k [00:00<00:00, 360kB/s]\n",
      "data/test-00000-of-00001.parquet: 100%|███████| 120k/120k [00:00<00:00, 180kB/s]\n",
      "Generating train split: 100%|████| 4957/4957 [00:00<00:00, 226992.65 examples/s]\n",
      "Generating validation split: 100%|██| 500/500 [00:00<00:00, 67910.75 examples/s]\n",
      "Generating test split: 100%|████████| 500/500 [00:00<00:00, 71624.04 examples/s]\n",
      "2026-01-16:16:02:20 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-16:16:02:20 INFO     [tasks:691] Task: openbookqa_urdu (openbookqa_urdu/direct/openbookqa_urdu.yaml)\n",
      "2026-01-16:16:02:20 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-16:16:02:20 INFO     [api.task:310] Building contexts for openbookqa_urdu on rank 0...\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 328.69it/s]\n",
      "2026-01-16:16:02:20 INFO     [evaluator:583] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                    | 0/40 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|████████████████████████| 40/40 [00:00<00:00, 9714.10it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/40 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, \u001b[A\n",
      "Processed prompts:   2%| | 1/40 [00:07<05:06,  7.87s/it, est. speed input: 18.68\u001b[A\n",
      "Processed prompts:  12%|▏| 5/40 [00:08<00:42,  1.20s/it, est. speed input: 89.86\u001b[A\n",
      "Processed prompts: 100%|█| 40/40 [00:08<00:00,  4.99it/s, est. speed input: 319.\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████| 40/40 [00:08<00:00,  4.99it/s]\n",
      "2026-01-16:16:02:29 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-16:16:02:29 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 32768}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: 10.0, num_fewshot: None, batch_size: auto\n",
      "|     Tasks     |Version|Filter|n-shot| Metric |   |Value|   |Stderr|\n",
      "|---------------|------:|------|-----:|--------|---|----:|---|-----:|\n",
      "|openbookqa_urdu|      1|none  |     0|acc     |↑  |  0.0|±  |0.0000|\n",
      "|               |       |none  |     0|acc_norm|↑  |  0.3|±  |0.1528|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm-eval --model vllm \\\n",
    "  --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=32768,trust_remote_code=True \\\n",
    "  --tasks openbookqa_urdu \\\n",
    "  --apply_chat_template \\\n",
    "  --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "  --batch_size 'auto' \\\n",
    "  --output_path './results/' \\\n",
    "  --log_samples \\\n",
    "  --limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f18a3769-ff60-40b0-939b-c028890ddca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16:15:37:17 WARNING  [config.evaluate_config:281] --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2026-01-16:15:37:17 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-16:15:37:56 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-16:15:39:24 INFO     [_cli.run:376] Selected Tasks: ['mgsm_cot_ur']\n",
      "2026-01-16:15:39:24 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-16:15:39:24 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-16:15:39:24 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 32768, 'trust_remote_code': True}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:30\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'trust_remote_code': True, 'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:33\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:33\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:33\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:43\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:43\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.158.2:53549 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:43\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:44\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:45\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:45\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.77it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.77it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:46\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.62 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:46\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.214310 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:51\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/637a19798a/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:51\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 4.61 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:54\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.138 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:54\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 5.75 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:55\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:55\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:55\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 40.15it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:58\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3028)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:58\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 11.64 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:39:58\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "[2026-01-16 15:40:08] WARNING repocard.py:106: Repo card metadata block was not found. Setting CardData to empty.\n",
      "2026-01-16:15:40:09 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-16:15:40:09 INFO     [tasks:691] Task: mgsm_cot_ur (mgsm_urdu_large/cot/mgsm_cot_ur.yaml)\n",
      "2026-01-16:15:40:09 INFO     [evaluator:313] mgsm_cot_ur: Using gen_kwargs: {'do_sample': True, 'until': ['سوال:', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-16:15:40:09 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-16:15:40:09 INFO     [api.task:310] Building contexts for mgsm_cot_ur on rank 0...\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 160.78it/s]\n",
      "2026-01-16:15:40:09 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|████████████████████████| 10/10 [00:00<00:00, 4453.02it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, \u001b[A\n",
      "Processed prompts:  10%| | 1/10 [00:04<00:39,  4.36s/it, est. speed input: 44.69\u001b[A\n",
      "Processed prompts:  30%|▎| 3/10 [00:04<00:08,  1.29s/it, est. speed input: 110.0\u001b[A\n",
      "Processed prompts:  40%|▍| 4/10 [00:05<00:06,  1.01s/it, est. speed input: 151.1\u001b[A\n",
      "Processed prompts:  50%|▌| 5/10 [00:06<00:05,  1.12s/it, est. speed input: 177.6\u001b[A\n",
      "Processed prompts:  60%|▌| 6/10 [00:18<00:17,  4.48s/it, est. speed input: 77.33\u001b[A\n",
      "Processed prompts:  70%|▋| 7/10 [01:48<01:34, 31.42s/it, est. speed input: 14.18\u001b[A\n",
      "Processed prompts:  80%|▊| 8/10 [02:36<01:13, 36.60s/it, est. speed input: 11.09\u001b[A\n",
      "Processed prompts:  90%|▉| 9/10 [03:43<00:46, 46.02s/it, est. speed input: 9.18 \u001b[A\n",
      "Processed prompts: 100%|█| 10/10 [03:54<00:00, 35.11s/it, est. speed input: 9.70\u001b[A\n",
      "Processed prompts: 100%|█| 10/10 [03:54<00:00, 23.40s/it, est. speed input: 9.70\u001b[A\n",
      "Running generate_until requests: 100%|██████████| 10/10 [03:54<00:00, 23.40s/it]\n",
      "2026-01-16:15:44:04 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-16:15:44:04 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 32768}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: 10.0, num_fewshot: None, batch_size: auto\n",
      "|   Tasks   |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|-----------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_cot_ur|      4|flexible-extract|     0|exact_match|↑  |    0|±  |     0|\n",
      "|           |       |strict-match    |     0|exact_match|↑  |    0|±  |     0|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm-eval --model vllm \\\n",
    "  --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=32768,trust_remote_code=True \\\n",
    "  --tasks mgsm_cot_ur \\\n",
    "  --apply_chat_template \\\n",
    "  --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "  --batch_size 'auto' \\\n",
    "  --output_path './results/' \\\n",
    "  --log_samples \\\n",
    "  --limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68921d94-c164-4f4b-b961-57ca4e2720ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16:15:00:35 WARNING  [config.evaluate_config:281] --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2026-01-16:15:00:35 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-16:15:02:00 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-16:15:05:06 INFO     [_cli.run:376] Selected Tasks: ['mgsm_direct_ur']\n",
      "2026-01-16:15:05:06 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-16:15:05:06 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-16:15:05:06 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384, 'trust_remote_code': True}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:13\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'trust_remote_code': True, 'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "config.json: 100%|█████████████████████████████| 679/679 [00:00<00:00, 5.25MB/s]\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:23\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:23\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:23\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "tokenizer_config.json: 3.07kB [00:00, 13.7MB/s]\n",
      "tokenizer.json: 7.03MB [00:00, 75.7MB/s]\n",
      "generation_config.json: 100%|██████████████████| 181/181 [00:00<00:00, 1.64MB/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:26\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:26\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.158.2:57025 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:26\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:27\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:49\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "model.safetensors: 100%|████████████████████| 3.55G/3.55G [00:03<00:00, 907MB/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:54\u001b[0m \u001b[90m[weight_utils.py:487]\u001b[0m Time spent downloading weights for deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B: 4.104034 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:54\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.87it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.87it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:55\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.59 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:05:55\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 27.784485 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:06:02\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/637a19798a/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:06:02\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 5.80 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:06:09\u001b[0m \u001b[90m[backends.py:261]\u001b[0m Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:06:15\u001b[0m \u001b[90m[backends.py:278]\u001b[0m Compiling a graph for compile range (1, 8192) takes 10.84 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:06:15\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 16.64 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:06:16\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:06:17\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:06:17\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 41.25it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:06:20\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1467)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:06:20\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 24.63 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:06:21\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "README.md: 2.56kB [00:00, 6.27MB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "[2026-01-16 15:06:27] WARNING repocard.py:106: Repo card metadata block was not found. Setting CardData to empty.\n",
      "mgsm/train-00000-of-00001.parquet: 100%|███| 9.10k/9.10k [00:00<00:00, 25.4kB/s]\n",
      "mgsm/test-00000-of-00001.parquet: 100%|█████| 87.8k/87.8k [00:00<00:00, 240kB/s]\n",
      "Generating train split: 100%|█████████████| 8/8 [00:00<00:00, 697.42 examples/s]\n",
      "Generating test split: 100%|████████| 250/250 [00:00<00:00, 38762.93 examples/s]\n",
      "2026-01-16:15:06:29 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-16:15:06:29 INFO     [tasks:691] Task: mgsm_direct_ur (mgsm_urdu_large/direct/mgsm_direct_ur.yaml)\n",
      "2026-01-16:15:06:29 INFO     [evaluator:313] mgsm_direct_ur: Using gen_kwargs: {'do_sample': True, 'until': ['سوال:', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-16:15:06:29 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-16:15:06:29 INFO     [api.task:310] Building contexts for mgsm_direct_ur on rank 0...\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 140.41it/s]\n",
      "2026-01-16:15:06:29 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                   | 0/10 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|████████████████████████| 10/10 [00:00<00:00, 4257.31it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, \u001b[A\n",
      "Processed prompts:  10%| | 1/10 [00:02<00:18,  2.05s/it, est. speed input: 115.1\u001b[A\n",
      "Processed prompts:  30%|▎| 3/10 [00:03<00:07,  1.01s/it, est. speed input: 201.6\u001b[A\n",
      "Processed prompts:  40%|▍| 4/10 [00:03<00:04,  1.35it/s, est. speed input: 234.1\u001b[A\n",
      "Processed prompts:  50%|▌| 5/10 [00:05<00:05,  1.19s/it, est. speed input: 197.3\u001b[A\n",
      "Processed prompts:  60%|▌| 6/10 [00:06<00:03,  1.07it/s, est. speed input: 216.8\u001b[A\n",
      "Processed prompts:  70%|▋| 7/10 [00:06<00:02,  1.30it/s, est. speed input: 227.8\u001b[A\n",
      "Processed prompts:  80%|▊| 8/10 [00:13<00:05,  2.57s/it, est. speed input: 120.9\u001b[A\n",
      "Processed prompts:  90%|▉| 9/10 [00:35<00:08,  8.52s/it, est. speed input: 49.68\u001b[A\n",
      "Processed prompts: 100%|█| 10/10 [02:14<00:00, 36.15s/it, est. speed input: 14.3\u001b[A\n",
      "Processed prompts: 100%|█| 10/10 [02:14<00:00, 13.42s/it, est. speed input: 14.3\u001b[A\n",
      "Running generate_until requests: 100%|██████████| 10/10 [02:14<00:00, 13.42s/it]\n",
      "2026-01-16:15:08:45 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-16:15:08:45 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: 10.0, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter      |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|-----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_direct_ur|      3|flexible-extract |     0|exact_match|↑  |  0.1|±  |   0.1|\n",
      "|              |       |remove_whitespace|     0|exact_match|↑  |  0.0|±  |   0.0|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm-eval --model vllm \\\n",
    "  --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=32768,trust_remote_code=True \\\n",
    "  --tasks mgsm_direct_ur \\\n",
    "  --apply_chat_template \\\n",
    "  --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "  --batch_size 'auto' \\\n",
    "  --output_path './results/' \\\n",
    "  --log_samples \\\n",
    "  --limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f84584-0a91-407b-97f0-f20760b80286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16:00:19:17 WARNING  [config.evaluate_config:281] --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2026-01-16:00:19:17 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-16:00:20:04 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-16:00:21:28 INFO     [_cli.run:376] Selected Tasks: ['minerva_math500_ur']\n",
      "2026-01-16:00:21:28 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-16:00:21:28 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-16:00:21:28 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384, 'trust_remote_code': True}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:33\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'trust_remote_code': True, 'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:34\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:34\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:34\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:36\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:37\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.16.64.2:47425 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:37\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:38\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:38\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:39\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.64it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.64it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:39\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.67 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:40\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.505162 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:45\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/93cbd4baf3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:45\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 5.10 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:49\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.277 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:49\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 6.38 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:49\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:50\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:50\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:02<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 39.24it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:53\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 4 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2137)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:54\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 13.52 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 00:21:54\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "Map: 100%|███████████████████████████| 500/500 [00:00<00:00, 6355.55 examples/s]\n",
      "2026-01-16:00:22:00 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-16:00:22:00 INFO     [tasks:691] Task: minerva_math500_ur (minerva_math_urdu/direct/minerva_math500_ur.yaml)\n",
      "2026-01-16:00:22:00 INFO     [evaluator:313] minerva_math500_ur: Using gen_kwargs: {'until': ['مسئلہ:'], 'do_sample': True, 'temperature': 0.6}\n",
      "2026-01-16:00:22:00 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-16:00:22:00 INFO     [api.task:310] Building contexts for minerva_math500_ur on rank 0...\n",
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 287.59it/s]\n",
      "2026-01-16:00:22:00 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                    | 0/5 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████████| 5/5 [00:00<00:00, 4242.67it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  20%|▏| 1/5 [00:03<00:15,  3.89s/it, est. speed input: 10.28 \u001b[A\n",
      "Processed prompts:  40%|▍| 2/5 [00:06<00:10,  3.41s/it, est. speed input: 19.37 \u001b[A\n",
      "Processed prompts:  60%|▌| 3/5 [00:07<00:04,  2.12s/it, est. speed input: 31.95 \u001b[A\n",
      "Processed prompts:  80%|▊| 4/5 [00:28<00:09,  9.74s/it, est. speed input: 13.70 \u001b[A\n",
      "Processed prompts: 100%|█| 5/5 [01:14<00:00, 22.60s/it, est. speed input: 10.87 \u001b[A\n",
      "Processed prompts: 100%|█| 5/5 [01:14<00:00, 14.87s/it, est. speed input: 10.87 \u001b[A\n",
      "Running generate_until requests: 100%|████████████| 5/5 [01:14<00:00, 14.87s/it]\n",
      "2026-01-16:00:23:17 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-16:00:23:17 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: 5.0, num_fewshot: None, batch_size: auto\n",
      "|      Tasks       |Version|Filter|n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|------------------|------:|------|-----:|-----------|---|----:|---|-----:|\n",
      "|minerva_math500_ur|      1|none  |     0|exact_match|↑  |  0.0|±  |0.0000|\n",
      "|                  |       |none  |     0|math_verify|↑  |  0.6|±  |0.2449|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm-eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=32768,trust_remote_code=True \\\n",
    "    --tasks minerva_math500_ur \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples \\\n",
    "    --limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bda24-dc92-40f1-97c1-b1d591ddffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e3f46b-25da-4891-a3ba-e6a6b8f94dba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16:15:22:06 WARNING  [config.evaluate_config:281] --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2026-01-16:15:22:06 INFO     [config.evaluate_config:301] Using default fewshot_as_multiturn=True.\n",
      "2026-01-16:15:22:47 INFO     [tasks:478] The tag 'truthfulqa_va' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2026-01-16:15:24:16 INFO     [_cli.run:376] Selected Tasks: ['mgsm_en_cot_en']\n",
      "2026-01-16:15:24:16 INFO     [evaluator:210] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-16:15:24:16 WARNING  [evaluator:222] generation_kwargs: {'do_sample': True, 'temperature': 0.6} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2026-01-16:15:24:16 INFO     [evaluator:235] Initializing vllm model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:22\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:22\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:22\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:22\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:23\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=1234, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:24\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.158.2:38197 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:24\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:24\u001b[0m \u001b[90m[gpu_model_runner.py:3562]\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:25\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:25\u001b[0m \u001b[90m[weight_utils.py:527]\u001b[0m No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.69it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.69it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:26\u001b[0m \u001b[90m[default_loader.py:308]\u001b[0m Loading weights took 0.65 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:26\u001b[0m \u001b[90m[gpu_model_runner.py:3659]\u001b[0m Model loading took 3.3466 GiB memory and 1.312681 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:31\u001b[0m \u001b[90m[backends.py:643]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/2eb9017ce7/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:31\u001b[0m \u001b[90m[backends.py:703]\u001b[0m Dynamo bytecode transform time: 4.76 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:34\u001b[0m \u001b[90m[backends.py:261]\u001b[0m Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:36\u001b[0m \u001b[90m[backends.py:278]\u001b[0m Compiling a graph for compile range (1, 8192) takes 2.60 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:36\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 7.35 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:37\u001b[0m \u001b[90m[gpu_worker.py:375]\u001b[0m Available KV cache memory: 35.20 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:37\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 1,318,032 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:37\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 131,072 tokens per request: 10.06x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:00<00:00, 41.33it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:40\u001b[0m \u001b[90m[gpu_model_runner.py:4587]\u001b[0m Graph capturing finished in 3 secs, took 0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2586)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:40\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 13.78 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-16 15:24:40\u001b[0m \u001b[90m[llm.py:360]\u001b[0m Supported tasks: ['generate']\n",
      "README.md: 15.0kB [00:00, 49.0MB/s]\n",
      "en/train-00000-of-00001.parquet: 100%|█████| 5.62k/5.62k [00:00<00:00, 13.9kB/s]\n",
      "en/test-00000-of-00001.parquet: 100%|██████| 39.9k/39.9k [00:00<00:00, 56.2kB/s]\n",
      "Generating train split: 100%|█████████████| 8/8 [00:00<00:00, 911.46 examples/s]\n",
      "Generating test split: 100%|████████| 250/250 [00:00<00:00, 54951.05 examples/s]\n",
      "2026-01-16:15:24:44 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-16:15:24:44 INFO     [tasks:691] Task: mgsm_en_cot_en (mgsm/en_cot/mgsm_en_cot_en.yaml)\n",
      "2026-01-16:15:24:44 INFO     [evaluator:313] mgsm_en_cot_en: Using gen_kwargs: {'do_sample': True, 'until': ['Question:', '</s>', '<|im_end|>'], 'temperature': 0.6}\n",
      "2026-01-16:15:24:44 WARNING  [evaluator:489] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\n",
      "2026-01-16:15:24:44 INFO     [api.task:310] Building contexts for mgsm_en_cot_en on rank 0...\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00, 89.49it/s]\n",
      "2026-01-16:15:24:45 INFO     [evaluator:583] Running generate_until requests\n",
      "Running generate_until requests:   0%|                    | 0/5 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████████| 5/5 [00:00<00:00, 2935.54it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  20%|▏| 1/5 [00:04<00:19,  4.88s/it, est. speed input: 12.08 \u001b[A\n",
      "Processed prompts:  40%|▍| 2/5 [00:05<00:06,  2.22s/it, est. speed input: 28.04 \u001b[A\n",
      "Processed prompts:  60%|▌| 3/5 [00:07<00:04,  2.11s/it, est. speed input: 27.26 \u001b[A\n",
      "Processed prompts:  80%|▊| 4/5 [00:33<00:11, 11.59s/it, est. speed input: 8.34 t\u001b[A\n",
      "Processed prompts: 100%|█| 5/5 [00:40<00:00, 10.05s/it, est. speed input: 10.16 \u001b[A\n",
      "Processed prompts: 100%|█| 5/5 [00:40<00:00,  8.13s/it, est. speed input: 10.16 \u001b[A\n",
      "Running generate_until requests: 100%|████████████| 5/5 [00:40<00:00,  8.13s/it]\n",
      "2026-01-16:15:25:26 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "2026-01-16:15:25:26 INFO     [loggers.evaluation_tracker:119] Saving per-task samples to results/deepseek-ai__DeepSeek-R1-Distill-Qwen-1.5B/*.jsonl\n",
      "vllm ({'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'dtype': 'auto', 'max_gen_toks': 16384}), gen_kwargs: ({'do_sample': True, 'temperature': 0.6}), limit: 5.0, num_fewshot: None, batch_size: auto\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|mgsm_en_cot_en|      3|flexible-extract|     0|exact_match|↑  |  0.8|±  |   0.2|\n",
      "|              |       |strict-match    |     0|exact_match|↑  |  0.0|±  |   0.0|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mgsm_en_cot_en\n",
    "!lm_eval --model vllm \\\n",
    "    --model_args pretrained=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,dtype='auto',max_gen_toks=16384 \\\n",
    "    --tasks mgsm_en_cot_en \\\n",
    "    --apply_chat_template \\\n",
    "    --gen_kwargs do_sample=true,temperature=0.6 \\\n",
    "    --batch_size 'auto' \\\n",
    "    --output_path './results/' \\\n",
    "    --log_samples \\\n",
    "    --limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de5a79-5d59-4222-a1fa-1ece1553110c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877cb973-629e-4bdc-8386-3e4907768215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e8753c-3ec9-4fd1-9b16-bb8530d90fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
